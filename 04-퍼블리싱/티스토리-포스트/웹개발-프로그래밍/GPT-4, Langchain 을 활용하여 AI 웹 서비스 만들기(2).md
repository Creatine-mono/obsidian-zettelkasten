# GPT-4, Langchain ì„ í™œìš©í•˜ì—¬ AI ì›¹ ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°(2)

> **ì›ë³¸ í¬ìŠ¤íŠ¸ ID**: 224
> **ë°œí–‰ì¼**: 2025-07-17 01:06:51
> **ì¹´í…Œê³ ë¦¬**: project/ì›¹ê°œë°œ

## ğŸ“ ì›ë¬¸ ë‚´ìš©

**Large language model import í•˜ê¸°**

from langchain.llms import OpenAI

from langchain.chat_models import ChatOpenAI

OpenAIì˜ ì†ŒìŠ¤ì½”ë“œ ì¤‘

client: Any = None #: :meta private:

model_name: str = Field(default="text-davinci-003", alias="model")

ChatOpenAIì˜ ì†ŒìŠ¤ì½”ë“œ ì¤‘

if self.tiktoken_model_name is not None:

model = self.tiktoken_model_name

else:

model = self.model_name

if model == "gpt-3.5-turbo":

# gpt-3.5-turbo may change over time.

# Returning num tokens assuming gpt-3.5-turbo-0301.

model = "gpt-3.5-turbo-0301"

**ë‘˜ì˜ ì°¨ì´ì ì„ ë¹„êµí•´ë³´ì**

![](./img/224_img.png) ![](./img/224_img_1.png)

ì´ì œí•œë²ˆ ì¨ë´…ì‹œë‹¤ 

llm = OpenAI(model_name="gpt-3.5-turbo-1106")

chat = ChatOpenAI(model_name="gpt-3.5-turbo")

  


a = llm.predict("Hello, how are you today?")

b = chat.predict("Hello, how are you today?")

  


a,b

**ê²°ê³¼ í™”ë©´**

![](./img/224_img_2.png)

ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚œë‹¤ë©´ 

.env íŒŒì¼ì— API KEY í™•ì¸í•˜ê¸°

OPENAI_API_KEY = sk-proj.......

ê¼­ ì´ëŸ°ì‹ìœ¼ë¡œ ë˜ì–´ì•¼í•¨

ë‹¤ë¥¸ ì˜µì…˜ë„ ìˆìŒ

llm = OpenAI(model_name="gpt-3.5-turbo-1106")

chat = ChatOpenAI(model_name="gpt-3.5-turbo")

ì—¬ê¸° ê´„í˜¸ì•ˆì— APIí‚¤ë¥¼ ë„£ëŠ” ë°©ë²• í•˜ì§€ë§Œ .envíŒŒì¼ì—ë‹¤ê°€ ë„£ëŠ”ê²Œ í¸í•©ë‹ˆë‹¤.

**Predict Messages**

from langchain.chat_models import ChatOpenAI

  


chat = ChatOpenAI(

temperature=0.1

)

ì•ì„œ ë³´ì‹œë‹¤ì‹œí”¼ ê´„í˜¸ì•ˆì—ëŠ” ì„¤ì •í• ìˆ˜ìˆëŠ”ê²Œ ë“¤ì–´ê°‘ë‹ˆë‹¤.

from langchain.schema import HumanMessage, AIMessage, SystemMessage

  


messages = [

SystemMessage(content="You are a friendly and concise language tutor. "),

AIMessage(content= "Use technical but understandable language suitable for a college student."),

HumanMessage(content="Can you explain the difference between BERT and GPT in simple terms?")

]

  


chat.predict_messages(messages)

**ë‹µë³€**
    
    
    AIMessage(content='Of course! BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are both advanced models used in natural language processing.\n\nThe main difference between BERT and GPT lies in their architecture and training objectives. BERT is designed for bidirectional learning, meaning it can understand the context of a word based on both the words that come before and after it. This allows BERT to capture more complex relationships within a sentence.\n\nOn the other hand, GPT is a unidirectional model that generates text based on the context it has seen so far. It excels at tasks like text generation and completion.\n\nIn summary, BERT focuses on understanding bidirectional context, while GPT is more geared towards generating text based on the context it has learned.')

**Prompt Templates**


## ğŸ”— ì œí…”ì¹´ìŠ¤í… ì—°ê²°

### ê´€ë ¨ ê°œë…
- [[]]
- [[]]

### í”„ë¡œì íŠ¸ ì—°ê²°
- [[]]

### í•™ìŠµ í¬ì¸íŠ¸
-

## ğŸ“‹ ì•¡ì…˜ ì•„ì´í…œ
- [ ]
- [ ]

## ğŸ’¡ ê°œì¸ì  í†µì°°



---

**íƒœê·¸**: #projectì›¹ê°œë°œ
**ìƒíƒœ**: ğŸŒ± ì”¨ì•— (ì œí…”ì¹´ìŠ¤í… í†µí•© ëŒ€ê¸°)
**ë³€í™˜ì¼**: 2025-10-07
