# 머신러닝과 딥러닝, 컴퓨터 비전, 자연어 처리

> **원본 포스트 ID**: 205
> **발행일**: 2025-07-03 18:28:36
> **카테고리**: project/AI 개발

## 📝 원문 내용

#### **머신러닝의 정의**

머신이 코드로 명시되지 않은 동작을 데이터로부터 학습하여 실행할 수 있도록 하는 알고리즘

\- 데이터로부터 일관된 패턴 또는 새로운 지식을 찾아내(학습하)는 방법

\- 학습된 알고리즘(Model)을 적용하여 정해진 업무를 처리

![](./img/205_img.png) https://devhwi.tistory.com/13

#### **머신러닝 알고리즘**

지도학습vs. 비지도학습

![](./img/205_img_1.png) https://wikidocs.net/258079

**머신러닝 알고리즘 종류**

| **알고리즘(Algorithm)**  
---|---  
**지도학습(Supervised Learning)** | **회귀분석(Regression Analysis)**  
**로지스틱 회귀(Logistic Regression)**  
**의사결정 나무(Deision Tree)**  
**랜덤 포레스트(Random Forest)**  
**부스팅(Boosting)**  
**K-최근접 이웃(K-Nearest Neighbors)**  
**신경망(Neural Network)**  
**비지도 학습(Unsupervised Learning)** | **K-평균 군집(K-means Clutering)**  
**주성분 분석(Principal Component Analysis)**  
**연관규칙(Assdciation Rules)**  
  
**딥러닝 정의**

인공신경망(Artificial Neural Network)

\- 인간의 뇌 구조를 모방하여 만들어짐(신경세포: Neuron)

![](./img/205_img_2.png)

**퍼셉트론**

  * 퍼셉트론(Perceptron): 인공신경망의 한 종류, 가장 간단한 형태의 Forward Network
  * 노드의입력값(Input)과 가중치(Weight)의 곱을 모두 합하여 활성화 함수를 통해 출력값 생성 - ŷ= sigmoid(WX + b)



**다층신경망**

  * Multi Layer Perceptron: 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)로 구성
  * 딥러닝: 은닉층이2개이상인인공신경망



![](./img/205_img_3.png) https://velog.io/@aosdbfc/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EC%B4%88-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EB%AA%A8%ED%98%95

#### **딥러닝의 Backpropagation Algorithm**

  * Forward propagation을수행하여y_hat계산- y와y_hat을사용하여오차값을계산
  * 학습: 오차값이 감소하는 방향으로 Parameter 수정
  * \- 효율적인 경사값(Gradient) 계산을 위하여 Backpropagation 수행
  * \- Parameter Update를 위해 출력층의 오차값을 은닉층으로 전달
  * 미분의 연쇄법칙 + 오차 역전파 알고리즘
  * \- 수치미분 과정 없이 학습에 필요한 경사값(Gradient) 계산
  * \- Hidden Layer나 Node가 증가해도 빠른 속도로 학습(Parameter Update) 가능



#### **딥러닝 알고리즘 종류**

**심층신경망(DNN: Deep Neural Network)**

입력층과 출력층 사이에 다수의 은닉층이 존재하는 일반적인 신경망 구조

**합성곱신경망(CNN: Convolutional Neural Network)**

이미지나 영상 데이터를 처리하기 위해 설계된 신경망으로, 합성곱 계층과 풀링 계층을 통해 공간적 특징을 학습

**순환신경망(RNN: Recurrent Neural Network)**

순차적 데이터(예: 시계열데이터)를 처리하며, 이전 상태의 정보를 현재 상태로 전달하는 구조

**생성적적대신경망(GAN: Generative Adversarial Network)**

두 개의 신경망(생성기와판별기)이 경쟁적으로 학습하여 데이터를 생성

**트랜스포머(Transformer)**

셀프어텐션(Self-Attention) 메커니즘을 사용하여 입력 데이터 간의 관계를 학습

#### **컴퓨터 비전(Computer Vision) 정의**

  * 인공지능(AI)의 한 분야
  * 컴퓨터 시스템을 통해 디지털 이미지, 비디오 및 기타 시각적 입력에서 의미있는 정보를 추출한 다음 이러한 정보를 바탕으로 다양한 작업을 실행
  * Computer Vision 종류


  1. Image Classification
  2. Object Localization
  3. Object Detection
  4. Image Segmentation



![](./img/205_img_4.png) https://m.blog.naver.com/dnjswns2280/222045224532

#### **이미지 분류 알고리즘**

입력된 이미지의 특징을 확인하여 분류 결과를 제공

합성곱 신경망: 여러개 Filter(Parameter Matrix)를 사용하여 이미지 특징(Feature Map) 추출

\- CNN Layer 말단에 DNN Layer를 적용하여 분류 및 예측 작업 수행

![](./img/205_img_5.png) https://medium.com/ddiddu-log

**합성곱신경망 과 일반신경망의 성능을 비교**

합성곱 신경망: 이미지 데이터를 처리하기 위해 설계된 신경망으로, 합성곱 계층과 풀링 계층을 통해 공간적 특징을 학습

\- 합성곱 연산 적용으로 가중치(Weight) 수 감소 및 성능 향상

![](./img/205_img_6.png)

#### **CNN 기반 이미지 분류 모델**

VGG16: 깊이가 깊은 합성곱 신경망(CNN)으로, 작은 3×3 필터를 여러 개 쌓아 이미지 분류 및 특징 추출에 활용

\- 주로 전이 학습(Transfer Learning)에서 사전 학습된 모델로 많이 사용됨

InceptionNet: 다양한 크기의 필터를 병렬로 사용하여 네트워크의 깊이를 효과적으로 증가시키는 구조

\- 이미지분류, 객체 탐지 등에서 높은 성능을 보이며, GoogLeNet으로도 알려짐

ResNet(Residual Network): 스킵연결(Skip Connection)을 도입한 모델

\- 딥러닝 모델의 깊이를 극단적으로 확장하면서도 성능 저하를 방지하는 데 활용됨

EfficientNet: 모델 크기(너비, 깊이, 해상도)를 균형 있게 확장하여 높은 성능과 효율성을 갖춘 CNN 모델

\- 모바일 및 임베디드 시스템에서도 활용될 만큼 연산량 대비 성능이 뛰어남

#### **객체 탐지 알고리즘**

![](./img/205_img_7.png) https://velog.io/@ekdl/Deep-Learning-6-%EA%B0%9D%EC%B2%B4%EC%9D%B8%EC%8B%9D%EB%AA%A8%EB%8D%B8YOLOv5-%EC%8B%A4%EC%8A%B5

**분류(Classification)**

인공지능은 데이터셋(데이터와 정답 레이블)을 함께 학습한 인공지능은 이를 토대로 새로운 이미지를 식별하게 되는 과정

단점 : 학습되지 않은 class는 인식하지 못함

**영역표시(Localiztion)**

Classification + Localization

분류를 통하여 검출한 객체의 정보가 있는 위치를 보기 쉽게 box형태로 지정하는 것을 Localiztion 이라고함

바운딩 박스(Bounding box) 학습을 통해 검출한 객체의 영역을 사각형으로 표시하느 것을 의미

단점 : 1가지 class에 적용 1개의 object

**객체탐지(Object Detection)**

Object Detection

학습을 통하여 여러 개의 객체를 인식하고 인식된 객체를 바운딩박스(Bounding box)와 색을 이용하여 영역을표시하는 과정

분류는 객체를 1개만 검출 객체 탐지는 객체를 1개 이상을 검출

단점 : 크기나 모양을 명확하게 탐지하지 못함

**분할(Segmentation)**

Instance Segmentantion

객체 탐제에서 이미지 내의 의미 있는 단위로 분할 하는 작업을 말함 

정교하고 복잡한 인공지능 구현을 위하여 이미지의 영역별 의미를 부여하는 경우 사용하는 방식

****

#### **자연어 처리 정의**

자연어처리: 학습된 모델로 문장 분류, 문장 요약, 문장 생성, 번역 등의 작업 수행

대량의 말뭉치(Corpus)를 딥러닝 모델 학습에 활용 

\- 말뭉치: 모델링을 위하여 특정 목적을 가지고 수집한 언어의 표본

언어(단어, 문장)에 존재하는 특징을 표현하기 위해 확률을 할당하는 것

\- 문장에서 다음에 나타날 단어를 예측하는 모델

문장(Sequence)이 적절한지, 말이 되는지 판단하기 위한 기준

\- P(승객이 버스에 탔다) vs. P(승객이 버스에 태운다)

\- 나는 딥러닝 알고리즘을~ P(배운다) vs. P(어렵다) vs. P(고친다) vs. P(가르친다)

#### **자연어 처리의 종류**

1\. 기계번역(Machine Translation)

\- 한 언어의 텍스트를 다른 언어로 번역

2\. 문장 생성(Text Generation)

\- 주어진 입력을 기반으로 새로운 문장을 생성

3\. 문서 요약(Document Summarization)

\- 긴 문서를 요약하여 핵심 내용을 추출

4\. 질의응답(Question Answering)

\- 질문에 대해 텍스트에서 답을 추출하거나 생성

5\. 감정 분석(Sentiment Analysis)

\- 텍스트의 감정을 긍정, 부정 또는 중립으로 분류

#### **토큰화 (Tokenization)**

텍스트 데이터를 분석하기 위해 문장을 작은 단위(토큰)로 분리하는 과정

영어에서는 공백이나 구두점으로 단어를 나누는 것이 일반적

한국어에서는 형태소 분석을 통해 어절이나 의미 단위로 나누는 경우가 많음

예시 문장: "나는 딥러닝을 배운다."

\- 공백 기반 토큰화: ["나는", "딥러닝을", "배운다."]

\- 형태소 기반 토큰화: ["나", "는", "딥러닝", "을", "배운", "다"]

한국어 토큰화 도구

\- KoNLPy (Hannanum, Kkma, Komoran, Mecab등)

\- Okt (Open Korean Text)

#### **벡터화(Vectorization)**

컴퓨터는 숫자 데이터를 처리하므로, 텍스트와 같은 비정형 데이터를 수치 데이터(벡터)로 변환이 필요

자연어 처리에서 벡터화는 텍스트를 딥러닝 알고리즘에 입력할 수 있도록 수치적 표현으로 바꾸는 작업

벡터화의 주요 방법

\- Bag of Words (BoW): 단어의 빈도를 기반으로 벡터를 생성

\- TF-IDF: 단어의 중요도를 반영하여 가중치를 부여한 벡터 생성

\- Word Embedding: 단어 간의 의미적 관계를 반영한 밀집 벡터 생성

예시 문장: "나는 딥러닝을 배운다."

\- BoW: [1, 1, 1, 0, 0] (단어 빈도 기반)

\- Word2Vec: [0.25, -0.12, ..., 0.89] (의미적 관계 반영)

**Word2vec 벡터화**

문장 내의 비슷한 위치(neighbor words)에 있는 단어로부터 유사도 획득

\- 각각의 단어 벡터가 단어 간 유사도를 반영한 값을 가지고 있음

분산 표현(Distributed Representation)

\- 분포 가설: 비슷한 위치에 등장하는 단어는 비슷한 의미를 가짐

[Korean Word2Vec](https://word2vec.kr/search/)

[ Korean Word2Vec QUERY RESULT word2vec.kr ](https://word2vec.kr/search/)

![](./img/205_img_8.png)

#### **자연어 처리 알고리즘**

순환 신경망(RNN: Recurrent Neural Network)

\- 순차적 데이터를 처리하며, 이전 상태의 정보를 현재 상태로 전달하는 구조(내부 루프가 존재)

**계층의 출력이 순환** 하는 인공신경망

순환방식은 은닉 계층의 결과가 다음 계층으로 넘어가며,자기 계층의 입력으로 다시 사용

**시계열 정보 처리** 처럼 앞뒤 신호의 상관도가 있는 경우

**음성, 동영상, 텍스트** 의 앞뒤를 분석하는 등 언어치리

순환신경망(RNN: Recurrent Neural Network)

\- 직전단계의기억(Short-Term Memory)을사용하여학습

![](./img/205_img_9.png) https://joonable.tistory.com/36

#### **LSTM 구조와 동작 원리**

Short-Term Memory만 사용하는 순환 신경망에서 Long-Term Dependency 문제 발생

LSTM(Long Short-Term Memory)

\- 기존 RNN에 Long-Term Memory(Memory Cell) 구조 추가

![](./img/205_img_10.png)


## 🔗 제텔카스텐 연결

### 관련 개념
- [[]]
- [[]]

### 프로젝트 연결
- [[]]

### 학습 포인트
-

## 📋 액션 아이템
- [ ]
- [ ]

## 💡 개인적 통찰



---

**태그**: #projectAI개발
**상태**: 🌱 씨앗 (제텔카스텐 통합 대기)
**변환일**: 2025-10-07
