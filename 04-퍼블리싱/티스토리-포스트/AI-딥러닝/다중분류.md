# 다중분류

> **원본 포스트 ID**: 132
> **발행일**: 2025-05-26 01:20:57
> **카테고리**: 추가공부/Easy! 딥러닝

## 📝 원문 내용

**다중분류**

다중 분류는 이진 부류의 개념을 확장한 것입니다.

예를 들어, 강아지와 고양이 사진만을 분류하던

이진 분류에서 나아가 강아지, 고양이, 소 사진을 분류하는 경우를 생각해볼 수 있습니다.

이처럼 세 개 이상의 클래스를 분류하는 경우를 다중분류라고 부릅니다.

**One-hot encoding**

"정답 클래스만 1, 나머지는 0으로 표시하는 벡터"

예시 : 클래스가 3개 있다고 할 때

클래스 이름 | 클래스 번호 | One-hot 벡터  
---|---|---  
고양이 | 0 | [1, 0, 0]  
개 | 1 | [0, 1, 0]  
토끼 | 2 | [0, 0, 1]  
  
이러한 레이블의 표현 방식은 각 클래스간에 "순서나 거리"를 강제로 만들지 않습니다.

만약 정답을 그냥 숫자(label) 0,1,2으로 표현한다면 모델:

클래스 0과 1은 가까운 관계고 0과 2는 멀리 떨어져 있다고 오해 할 수 있습니다.

이진 분류에서는 출력층에 노드 1개만 필요했지만, 다중 분류에서는 레이블이 벡터로 표현됨에 따라 분류해야 하는 클래스의 개수만큼의 노드가 필요합니다.

만약 출력이 [0, 100, 0], [-1, 1, -1]과 같이 나온다면?

이는 [0, 1, 0]보다 두 번째 노드의 값이 더 두드러지게 나온 결과값임에도 불구하고,

실제 레이블과는 더 멀어진 결과이므로 Loss 값은 [0, 1, 0]이 나왔을 때 보다 더 커지게 됩니다.

이러한 문제를 해결하기 위해, 각 출력값은 양수이면서 그 합이 1이 되도록 제한을 둡니다.

이렇게 하면 [0, 100, 0]이나 [-1, 1, -1]과 같은 출력은 나오지 않습니다.이를 위한 액티베이션이 바로 Softmax입니다.

**Softmax**

Softmax는 여러 개의 실숫값을 입력받아 각 출력의 값이 양수이면서 그 합이 1이 되도록 변환하는 함수입니다.

**Softmax** 는 다중 분류 문제에서 가장 많이 사용되는 **출력 함수(활성화 함수)** 중 하나입니다.  
이 함수는 **모델의 출력(logits)을 확률처럼 보이도록 바꿔주는 역할** 을 합니다.

![](./img/132_img.png)

다른 액티베이션들과 달리, Softmax는 개별 노드의 값이 아닌 모든 출력노드의 값을 동시에 고려합니다.

따라서 위 그림과 같이 하나의 상자로 표현합니다.

어차피 확률로 표현할꺼면 각 노드마다 Sigmiod하면 되는거아닌가라는 의문이 드는데 이때 는 각 노드를 개별로 고려한것이므로 출력값들의 합이 1이 넘어가서 확률로 표현하지 못합니다.

**수식**

![](./img/132_img_1.png)

  * z는 실수값의 입력 벡터 
  * K는 분류 클래스 수
  * e는 자연로그(오일러 수)의 밑



![](./img/132_img_2.png) https://tiabet0929.tistory.com/63

참고자료의 plot은 softmax함수의 개형을 위처럼 설명하지만 정확히 따지자면 이렇다.

![](./img/132_img_3.png) https://botpenguin.com/glossary/softmax-function

**Cross-Entropy Loss**

Cross-Entropy Loss는 "예측이 정답과 얼마나 가까운가?"를 평가합니다.

  * 정답에 가까운 확률을 예측하면 손실 값이 **작아집니다**.
  * 틀린 값을 자신 있게 예측하면 손실 값이 **매우 커집니다**



**수식**

다중 클래스 분류 (softmax 출력 사용시):

![](./img/132_img_4.png)

  * C: 클래스 수
  * yi​: 실제 레이블 (one-hot encoding)
  * y^i​: 모델이 예측한 확률



예: 정답이 클래스 3이고, softmax 결과가 [0.1, 0.2, 0.7]이면,

![](./img/132_img_5.png)

예시)

선생님이 문제를 내고,학생들이 정답을 맞추는 방식으로 예시를 들어보겠습니다

여기 피자,햄버거,치킨이 있다고 칩시다.

이때 피자, 햄버거, 치킨을 각각 가려두고 선생님만 정답을 알고있는채로

가려둔 접시중 하나를 확률로 표현해서 맞춰보라고 하는데 학생들은 이문제를 꼭 맞춰야되는 상황입니다.

벌점은 낮을수록 좋은거고 틀리면 벌점이 커지는 겁니다.

정답: 피자

학생1

  * 햄버거: 10% 확률
  * 피자: 85% 확률
  * 치킨: 5% 확률



→ 피자를 맞추고 확신도 있었으니까 벌점 낮음 

학생2

  * 햄버거: 90% 확률 
  * 피자: 5% 확률
  * 치킨: 5% 확률



→ 피자를 맞추지도 못하고 햄버거를 자신감있게 말했으니까 벌점 많이 준다.

이러한 방법으로 컴퓨터를 학습을 시켜가며 똑똑해지도록 연습을 시킬수있습니다.


## 🔗 제텔카스텐 연결

### 관련 개념
- [[]]
- [[]]

### 프로젝트 연결
- [[]]

### 학습 포인트
-

## 📋 액션 아이템
- [ ]
- [ ]

## 💡 개인적 통찰



---

**태그**: #추가공부Easy!딥러닝
**상태**: 🌱 씨앗 (제텔카스텐 통합 대기)
**변환일**: 2025-10-07
