# 기계학습개론 1학기 4,5주차 (오버피팅,언더피팅 피하는 기술 & 딥러닝 최적화 기법)

> **원본 포스트 ID**: 12
> **발행일**: 2025-04-06 20:58:18
> **카테고리**: College study/Machine learning basics

## 📝 원문 내용

#### **1\. 과대적합**

▷ AI 모델이 학습 데이터에 지나치게 맞추어져 모델이 생성되었을 때, 새로운 데이터에 대해서는 일반화(generalization)능력이 부족한 상태을 의미

▷ AI 모델이 훈련 데이터에 대해서는 높은 정확도를 보이지만,테스트 데이터에 대해서는 성능(분류,예측)이 낮아지는 현상 

#### **1\. 과소적합**

▷ AI 모델이 학습 데이터를 제대로 학습하지 못하여,훈련 데이터에 대한 정확도가 낮은 상태를 의미

▷ 이 경우에는 새로운 데이터(테스트 데이터)나 실제 데이터(학습 데이터)에서도 성능 저하 우려

규제의 필요성

구분 | 설명 | 문제점 | 해결  
---|---|---|---  
오버피팅 | 너무 외워버림  | 일반화 안 됨 | 단순화, 정규화  
언더피팅 | 공부 부족 | 학습 자체를 못함 | 모델 복잡도 증가,학습 더 하기  
  
![](./img/12_img.jpg) https://wikidocs.net/152777

#### **2\. 과대적합 과 과소적합**

과대적합과 과소적합은 모델의 복잡도와 학습 데이터의 양에 영향을 받음

\- 모델이 복잡하면 학습 데이터에 과대적합될 가능성이 높음

\- 모델이 단순하면 학습 데이터에 과소적합될 가능성이 높음

\- 학습 데이터의 양이 부족하면 모델이 학습 데이터에 과대적합될 가능성이 높음

#### **2(1). 과대적합에 빠지는이유**

모델이 훈련 데이터에 너무 맞춰져있고, 새로운 데이터에 대한 일반화 능력이 떨어짐

▷ overfitting (일반화 능력이 없어짐)

Training Dataset을 단순히 '암기'하는 과대적합에 주의해야 함

![](./img/12_img.png) https://ukb1og.tistory.com/28

최적점에서 끊고 그냥 내야함

규제의 필요성

그래서 과대적합을 피하기위해서 학습 과정에서 여러 규제기법을 적용해야함

![](./img/12_img_1.png)

과적합 방지 전략들

**(1) Weight Decay - 가중치 감소**

큰 가중치 값을 줄여서 모델 복잡도를 제한

ex) MSE

![](./img/12_img_2.png)

가중치들이 커질수록 패널티를 더 크게줌

손실함수에 가중치 항을 추가해 패널티 부여

보통 L2 Norm 사용

**(2) Early Stopping - 조기 종료**

검증 데이터에서 성능이 더 이상 좋아지지 않으면 학습 중단

![](./img/12_img_3.png)

Epoch 수를 너무 키우면 과적합이 생김

일정 조건(성능 정체)에서 학습 중단 → 시간도 절약

조기 종료 기준 

모델의 성능이 바로 향상하지 않는다고 종료해버리면 학습이 제대로 되지 않을 수 있음.

**(3) Data Augmentation - 데이터 증대**

기존 데이터를 변형하여 학습 데이터를 증가

![](./img/12_img_4.png) 데이터가 커지면서 과대적합이 자연스럽게 사라지는 현상

회전,반전,크기 조절,잡음 추가 등으로 이미지 다양성 확보

CNN에서 특히 자주 쓰임

**(4) Dropout - 드롭아웃**

신경망 전체를 다 학습기키지 않고 일부 노드만 무작위로 골라 학습시키는 기법

![](./img/12_img_5.png)

매번 다른 구조로 학습되므로 앙상블 효과 있음

과적합 억제에 효과적

위치 : ReLU → Dropout → pooling 순서로 배치

딥러닝 최적화 기법

#### **데이터 전처리 (Pre-processing)**

필요한 이유

  * 특성(feature)마다 단위, 크기, 범위가 너무 다르면
  * 학습 시 편향된 업데이트가 일어나서 학습 속도도 느려지고, 성능도 나빠짐



ex) 건강에 관련된 데이터 (키(m),몸무게(kg),혈압)

키 : 1.885m와 1.525m는 차이는 33cm/ 특징값을 미터로 기준값 설정했을 때,특징값 차이는 0.33

→ 단위 cm 와 m에 대한 각 특징값 차이는 대략 100배의 규모 차이를 보임

  * 편차규모가 큰 항목에 대해 가중치가 크게 실림
  * Backpropagation (역전파) 진행할 때 몸무게 미분값이 크니까 수치가 빠르게 갱신
  * 편차가 적은 키는 미세하게 변형되므로 계산(학습)이 느리게 이루어짐



**방향성 문제**

Gradient는 그 지점에서의 기울기 방향

우리가 가야 하는 방향은 기울기의 반대 방향

![](./img/12_2dKCQHh.gif) alec radford

만약 손실함수지형이 이 사진과같이 생겼으면

y축 방향 기울기는 매우 크니까 커다란 step으로 튀지만 

x축 방향은 작으니까 조금씩만 이동

결과적으로 좌우로 왔다 갔다 하면서 느리게 이동하게되는 문제점이 나타남

**정규화 (Normalization)**

데이터 값들을 공통된 규정/규범의 간격에 맞게 변경

Min-Max 정규화

모든 데이터를 0~1사이로 바꿔주는 작업

![](./img/12_img_6.png)

**표준화 (Standardization)**

데이터를 평균 0,표준편차 1로 맞춤

![](./img/12_img_7.png)

**명칭값 (Nominal value)**

one-hot 코드로 변환 

Ex) 성별의 남(1)과 여(2), 체질의 태양인(1), 태음인(2), 소양인(3), 소음인(4)   
성별은 2비트, 체질은 4비트 부여

![](./img/12_img_8.png)

**가중치 초기화 (Weight Initialization)**

  * 가중치를 0이나 같은 값으로 초기화하면 학습이 멈춤
  * 따라서 랜덤값으로 초기화, 다만 너무 크거나 작으면 학습이 안 됨.



가중치 초기화 코드 실행해보기
    
    
    import numpy as np
    import matplotlib.pyplot as plt 
    
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
    
    x = np.random.randn(1000, 100)  # 1000개의 입력 데이터, 100차원
    
    node_num = 100      # 각 은닉층의 노드 수
    hidden_layer_size = 5  # 오타 있었음: sioze → size
    activations = {}    # 이곳에 활성화값을 저장
    
    for i in range(hidden_layer_size): 
        if i != 0:
            x = activations[i-1]
    
        # 가중치 초기화 방법 중 하나 사용 (여기서는 Xavier)
        w = np.random.randn(node_num, node_num) / np.sqrt(node_num)
    
        a = np.dot(x, w)
        z = sigmoid(a)
        activations[i] = z
    
    # 히스토그램 출력
    for i, a in activations.items():
        plt.subplot(1, hidden_layer_size, i + 1)
        plt.title(str(i + 1) + '-layer')
        plt.hist(a.flatten(), bins=30, range=(0, 1))
    
    plt.show()
    
    
    plt.show()

구글 Colab으로 돌려보기

![](./img/12_img_9.png)

**경사하강법 & 모멘텀 (Gradient Descent & Momentum)**

Gradient Descent 

  * 손실함수, 미분값 (기울기)이 최소가 되는 지점에 알맞은 가중치 매개변수를 찾아냄 
  * 손실함수의 경사 반대방향으로 정의된 Step size에 따라 움직이면서 최적의 파라미터를 찾으려 함 
  * 경사하강법에서는 학습 시 step size 중요



![](./img/12_img1.daumcdn.png) https://xiangddun.tistory.com/54

**학습률이 너무 작을 경우, (step size가 작을 수록)**  
➜ 알고리즘이 수렴하기 위해 반복해야 하는 값이 많으므로 학습시간이 오래걸림.  
➜ 지역 최소값(local minimum)에 수렴할 수 있음.

  
**학습률이 너무 클 경우, (step size가 클수록)**  
➜ 학습 시간은 적게 걸림.  
➜ 스텝이 너무 커서 전역 최소값(global minimum)을 가로질러 반대편으로 건너뛰어 최소값에서 멀어질 수 있음.

**Gradient Descent**

  * 경사하강법은 현재 위치에서 기울기를 사용하기 때문에 지역 최소값에 빠질 수 있음 
  * 무작위 초기화 (random initialization)로 인해 알고리즘이 전역 최소값이 아닌 지역 최소값에 수렴할 수 있음



![](./img/12_img_10.png)

**Momentum**

  * 학습 방향이 바로 바뀌지 않고, 일정한 방향을 유지하려는 성질
  * 같은 방향의 학습이 진행된다면 가속을 가지며 더 빠른 학습을 기대할 수 있음



**Momentum = Negative of Gradient + Momentum**

  1. 기울기에 관성을 부과하여 작은 기울기는 쉽게 넘어갈 수 있도록 만들어 줌 
  2. 공의 관성을 이용하여 쉽게 넘어갈 수 있게 하여 지역 최소값을 탈출 
  3. 모멘텀을 사용하지 않으면 기울기가 매우 작은 구간을 빠져나오는데 아주 오랜 시간이 걸림



![](./img/12_img_11.png)

**적응적 학습률 (Adaptive Methods)**

  * SGD: 네트워크 상 모든 개별 가중치는 동일한 학습률 사용 
  * Adagrad: 이전 gradient 를 누적한 정보 벡터 r를 이용, 이전 gradient 누적값이 크면, 다음에는 파라미터가 조금만 이동하고, 반대로 누적값이 작으면 많이 이동 
  * RMSprop: 이전 gradient를 누적할 때 오래된 것의 영향을 지수적으로 줄이기 위해 가중 이동평균 기법을 사용하여 Adagrad를 개선한 기법 
  * Adam: RMSProp에 모멘텀을 적용하여 RMSprp를 개선한 기법, 가장 일반적으로 사용되는 기법 배치 정규화



![](./img/12_image.gif) https://velog.io/@freesky/Optimizer

**Epoch (에포크)**

훈련 데이터 셋에 포함된 모든 데이터들이 한 번씩 모델을 통과한 횟수   
학습데이터 전체를 한번 학습하는 것 

1 epoch

전체 학습 데이터 셋이 한 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한 번 통과했다는 의미   
10 epoch 은 동일한 학습 데이터 셋을 10회 모델에 학습시켰다는 의미

![](./img/12_img_12.png) https://www.slideshare.net/slideshow/ss-82372826/82372826

epoch 값이 높을수록 다양한 무작위 가중치로 학습하였다는 의미   
지나치게 epoch이 올라가면 학습 데이터 셋에 대한 과적합 발생

**Batch Normalization**

각 층의 입력을 **미니배치 단위로 평균 0, 분산 1** 로 정규화시켜줌!

Internal Covariate Shift 현상 (내부 공변량 이동)

  * Batch 단위로 학습을 하게 되면 발생하는 문제점 존재 
  * 학습 과정에서 계층 별로 입력의 데이터 분포가 달라지는 현상 
  * 한 레이어 마다 입력과 출력을 가지고 있는데 이 레이어들끼리 covariate shift가 발생 
  * 레이어가 깊어질 수록 분포 형태가 더 변화가 심함 → Batch Normalization을 이용 



![](./img/12_img_13.png) https://gaussian37.github.io/dl-concept-batchnorm/

**ReLU (Rectified Linear Unit) 활성함수**

tanh(x) 함수 (시그모이드와 비슷) 를 사용하면 미분값의 범위가 확장   
고차원 데이터를 다룰 경우에는 값이 커질 수 있어 다시 경사가 소실될 수 있음   
복잡한 데이터일수록 고차원일 경우가 많은데 이를 회피할 수 있는 활성화 함수가 ReLU 함수

![](./img/12_img_14.png)

  * ReLU 함수는 x (입력값)가 0보다 작을 때 (음수)는 모든 값을 0으로 출력 
  * ReLU 함수는 x (입력값)가 0보다 클 때 (양수)는 입력값을 그대로 출력 
  * ReLU 함수는 x가 0보다 크기만 하면 미분값이 1이 되기 때문에 여러 은닉층을 거쳐도 맨 처음 층까지 사라지지 않고 남아있음   
딥러닝의 발전에 기여!!!



**학습의 비선형성**

  * 딥러닝 네트워크에서는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 주로 비선형 함수를 통과시킨 후 전달함 
  * 이때 사용하는 함수를 활성화 함수(Activation Function) 이라함 
  * **_비선형 함수를 사용하는 이유는 선형함수를 사용할 시 층을 깊게 하는 의미가 줄어들기 때문_**
  * 신경망에서는 활성화 함수로 선형함수가 아닌 비선형함수를 사용해야 함 
  * 히든레이어를 여러 개 다층으로 구성하고 활성화함수로 모두 선형함수를 이용하는 경우 단층으로 구성한 것과 정확하게 동일하게 구성할 수 있음. （무의미함 ） 
  * 따라서 깊게 망을 구성하려면 1개 이상의 비선형함수를 이용하여야 함 
  * 비선형성이 증가한다는 것은 그만큼 복잡한 패턴을 좀 더 잘 인식할 수 있게 된다는 의미 
  * 네트워크에 비선형성을 더해주기 위함




## 🔗 제텔카스텐 연결

### 관련 개념
- [[]]
- [[]]

### 프로젝트 연결
- [[]]

### 학습 포인트
-

## 📋 액션 아이템
- [ ]
- [ ]

## 💡 개인적 통찰



---

**태그**: #CollegestudyMachinelearningbasics
**상태**: 🌱 씨앗 (제텔카스텐 통합 대기)
**변환일**: 2025-10-07
