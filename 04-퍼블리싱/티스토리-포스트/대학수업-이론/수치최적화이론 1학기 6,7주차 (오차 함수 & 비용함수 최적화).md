# 수치최적화이론 1학기 6,7주차 (오차 함수 & 비용함수 최적화)

> **원본 포스트 ID**: 41
> **발행일**: 2025-04-21 04:07:29
> **카테고리**: College study/Numerical optimization

## 📝 원문 내용

#### **오차 함수 종류**

**1) 제곱 오차 (Squared Error) → 회귀 문제에서 사용**

![](./img/41_img.png)

**2) 교차 엔트로피 (Cross Entropy) → 분류 문제에서 사용**

![](./img/41_img_1.png)

언제 어떤 오차함수를 써야하는지

상황 | 오차 함수  
---|---  
결과가 숫자(연속 값) → 예: 키 예측 | 제곱 오차  
결과가 클래스(0/1, 고양이/개) | 교차 엔트로피  
  
앞에서 배운 걸 어떻게 학습에 실제로 써먹는지 

핵심 개념 | 한 줄 요약  
---|---  
로그 함수 | 오차를 계산할 때 씀  
교차 엔트로피 | 분류 문제에서 오차 계산하는 공식  
최대우도추정 (MLE) | 가장 그럴득한 확률을 찾는 수학적 방식  
비용함수 (Cost Function) | 오차를 숫자로 표현한 것 → 줄이는 게 목표  
파라미터 최적화 | 가중치와 바이어스를 업테이트하면서 정답에 가까워지기  
  
**로그함수**

로그함수는 x가 1에 가까울수록 0,x가 작아질수록 음수로 커짐

![](./img/41_Logarithms.svg.png)

ex)

예측값이 0.9 → log(0.9) ≈ -0.1

예측값이 0.1 → log(0.1) ≈ -1

정답확률이 낮으수록 값은 더 커짐

**교차 엔트로피(Cross Entropy)**

정답과 예측이 얼마나 다른지 계산하는 방법 (분류 문제)

수식

![](./img/41_img_2.png)

ex)

정답 y | 예측 y^  | 계산  
---|---|---  
[1, 0] | [0.9, 0.1] | −log⁡(0.9) ≈ 0.105  
[1, 0] | [0.5, 0.5] | −log⁡(0.5) = 0.693  
[1, 0] | [0.1, 0.9] | −log⁡(0.1) = 1.0  
  
정답에서 멀어질수록 오차가 커짐

**MLE (Macimum Likeihood Esimation)**

관측된 결과를 가장 잘 설명해주는 "확률"을 찾는 방법

Cost Function (비용함수)

신경망의 학습 목표 = 이 비용함수 값을 최대한 작게 만드는 것

**신경망 학습 전체 흐름**

  1. 입력 xxx를 넣고
  2. 계산: z=wx+b, a=σ(z)
  3. 예측값 y^​ 나옴
  4. 정답 y와 비교해서 오차 계산
  5. 오차를 줄이도록 w,b를 바꿔감  
  





## 🔗 제텔카스텐 연결

### 관련 개념
- [[]]
- [[]]

### 프로젝트 연결
- [[]]

### 학습 포인트
-

## 📋 액션 아이템
- [ ]
- [ ]

## 💡 개인적 통찰



---

**태그**: #CollegestudyNumericaloptimization
**상태**: 🌱 씨앗 (제텔카스텐 통합 대기)
**변환일**: 2025-10-07
