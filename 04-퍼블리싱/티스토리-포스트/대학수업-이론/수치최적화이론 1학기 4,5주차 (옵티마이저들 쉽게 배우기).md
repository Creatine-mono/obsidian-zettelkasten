# 수치최적화이론 1학기 4,5주차 (옵티마이저들 쉽게 배우기)

> **원본 포스트 ID**: 40
> **발행일**: 2025-04-21 03:29:32
> **카테고리**: College study/Numerical optimization

## 📝 원문 내용

![](./img/40_image.png)

**Gradient Descent (GD)**

모든 자료를 다 검토해서 내위치의 산기울기를 계산해서 갈 방향을 찾는거에서 생기는 문제점

  1. 데이터가 클수록 매번 전체 데이터를 계산 → 느림
  2. 진동(Oscillation) : 경사 방향이 바뀌면 계속 흔들림
  3. 학습률이 고정 : 모든 파라미터에 같은 크기 적용



**1\. Stochastic Gradient Descent (SGD)**

밥 다 안 먹고 "한 입만"먹으면서 학습

전체 데이터을 계산하는게 아니라 1개 또는 소수 샘플(batch)로 계산

개선된점

  * 계산 속도 훨씬 빠름
  * 큰 데이터셋에도 적합



단점 

  * 그래디언트가 매번 불안정 → 진동, 수렴 늦어짐 가능성 있음



![](./img/40_-44-2048.webp) https://www.slideshare.net/slideshow/ss-79607172/79607172 ![](./img/40_-45-2048.webp) https://www.slideshare.net/slideshow/ss-79607172/79607172

**2\. Momentum**

자전거 타고 속도 붙은 방향으로 가기

개선된점 

문제 | Momentum의 해결 방식  
---|---  
진동 | 이전 방향을 누적해서,급격한 방향 전환 억제  
느린 수렴 | 속도 가속도 처럼 누적되어 평평한 구간에서 더 빠름  
  
![](./img/40_img1.daumcdn.jpg) https://wikidocs.net/152765

**3\. Nesterov Accelerated Gradient (NAG)**

미래를 미리 내다보고 계산하기

현재 위치가 아니라 예상되는 미래 위치에서 그래디언트 계산

문제 | NAG의 해결 방식  
---|---  
진동/과도한 이동 | 이동 전에 "미리 보기"하면서 조절  
더 빠른 수렴  | 미래 예측을 통해 불필요한 튐 방지  
  
**4\. Adagrad**

자주 틀리니까 학습하지마

자주 바뀌는 변수 → 학습률 줄임

잘 안 바뀌는 변수 → 학습 유지

개선된점

문제 | Adagrad의 해결 방식  
---|---  
고정 학습률 | 자주 업데이트 되는 파라미터  
학습률 자동조절  | 매개변수마다 다른 학습률 적용됨  
  
단점 

그래디언트 제곱의 누적합이 계속 커지면 학습률이 0에 가까워져서 멈춤

**5\. RMSProp**

문제  |  RMSProp의 해결 방식   
---|---  
Adagrad의 학습 정지  |  오래된 제곱값은 줄이고, 최근 것만 반영   
학습률 안정화  |  너무 작아지지 않음, 계속 학습 가능   
  
Adagrad 보완 버전

**6\. Adam**

![](./img/40_img.gif) https://dbstndi6316.tistory.com/297

모든 기능 다 넣은 완전체

개선된 점

문제 |  Adam의 해결 방식   
---|---  
진동 | 모멘텀 적용으로 안정화  
학습률 문제 | RMSProp 기반으로 자동 조절  
빠른 수렴 | 이론상 많은 상황에서 좋은 성능  
  
관련 논문

<https://arxiv.org/abs/1412.6980>

[ Adam: A Method for Stochastic Optimization We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory r arxiv.org ](https://arxiv.org/abs/1412.6980)


## 🔗 제텔카스텐 연결

### 관련 개념
- [[]]
- [[]]

### 프로젝트 연결
- [[]]

### 학습 포인트
-

## 📋 액션 아이템
- [ ]
- [ ]

## 💡 개인적 통찰



---

**태그**: #CollegestudyNumericaloptimization
**상태**: 🌱 씨앗 (제텔카스텐 통합 대기)
**변환일**: 2025-10-07
