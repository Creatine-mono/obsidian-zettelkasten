# 딥러닝 기반 디지털 이미지 워터마킹 시스템 구현 보고서

## 1. HiDDeN 논문의 핵심 아이디어와 구성 요소

Facebook AI Research의 **HiDDeN** 논문은 딥러닝을 활용하여 이미지에 보이지 않는 정보를 은닉(스테가노그래피/워터마킹)하는 **최초의 종단간(trainable end-to-end)** 프레임워크를 제시했다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=We%20introduce%20HiDDeN%2C%20the%20first,We%20model%20this%20by). 이 기법의 핵심 아이디어는 **인코더-노이즈-디코더**로 이어지는 세 가지 신경망을 통해 원본 이미지를 인간의 시각으로는 거의 동일하게 유지하면서도, 해당 이미지에 비밀 정보를 은닉하는 것이다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,high%20accuracy%20by%20the%20decoder)[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,being%20detected%20by%20the%20adversary). 이를 위해 인코더가 출력한 _워터마크 삽입 이미지_는 사람 눈에 원본과 구별할 수 없지만, 디코더 신경망을 통해 삽입된 메시지를 정확히 복원할 수 있다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,high%20accuracy%20by%20the%20decoder). 또한 훈련 과정에서 **노이즈 계층(Noise Layer)**을 삽입하여 이미지 압축, 필터링 등 다양한 **왜곡**을 시뮬레이션함으로써, **워터마크의 강인성**(robustness)을 높이는 것이 특징이다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=We%20introduce%20HiDDeN%2C%20the%20first,We%20model%20this%20by). 아래 그림은 HiDDeN 모델의 전체 구조를 나타낸다.

_그림 1: HiDDeN 워터마킹 모델의 구조. 인코더(E)는 원본 이미지와 비밀 메시지(예: ID)를 입력받아 워터마크가 삽입된 이미지(스테고 이미지)를 출력한다. 이어서 노이즈 계층(N)이 스테고 이미지를 왜곡하여 전송 시 발생할 수 있는 손상을 모사하고, 디코더(D)는 노이즈 후의 이미지를 입력받아 숨겨진 메시지를 복원한다. 훈련 단계에서는 (a) 원본 이미지와 스테고 이미지 간 차이(**이미지 손실** L<sub>I</sub>), (b) 원본 메시지와 복원된 메시지 간 차이(**메시지 손실** L<sub>M</sub>), (c) 스테고 이미지를 탐지하려는 판별기(**적대적 손실** L<sub>adv</sub>)의 세 가지 손실을 조합하여 인코더와 디코더를 최적화한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,being%20detected%20by%20the%20adversary)[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Encoded%20images%20should%20look%20visually,of%20input%20messages%20and%20images)._

HiDDeN 모델은 **인코더(Encoder)**, **디코더(Decoder)**, **노이즈 계층(Noise Layer)**의 세 구성 요소를 중심으로 동작하며, 훈련 시 **적대적 판별기(Discriminator)**를 추가로 활용한다. 각 구성 요소의 역할은 다음과 같다:

- **인코더**: 원본 **커버 이미지**와 숨길 **비밀 메시지**(이진 문자열)를 입력받아 동일한 크기의 **스테고 이미지**(워터마크 삽입 이미지)를 출력한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=We%20introduce%20HiDDeN%2C%20the%20first,We%20model%20this%20by). 인코더가 만들어낸 결과물은 시각적으로 원본과 거의 구별되지 않도록 훈련된다.
    
- **노이즈 계층**: 인코더 출력에 대해 **JPEG 압축**, **가우시안 블러**, **드롭아웃**, **크롭(crop)** 등 다양한 **이미지 손상**을 모방하는 계층이다. 이를 통해 실제 SNS 업로드 과정에서 일어날 수 있는 품질 저하를 네트워크가 견딜 수 있도록 한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=decoder%20network%20receives%20the%20encoded,We%20model%20this%20by)[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=The%20Identity%20layer%20is%20the,the%20distortion%3A%20Dropout%2C%20Cropout%2C%20and). (노이즈 계층은 학습 시에만 사용되며, 추론 시에는 실제로 발생한 손상이 그대로 입력된다.)
    
- **디코더**: (노이즈를 거친) 워터마크 이미지를 입력받아 숨겨진 **메시지 비트열**을 복원해낸다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,high%20accuracy%20by%20the%20decoder). 디코더의 출력은 인코더에 넣었던 원래 메시지와 동일한 형식(ID 문자열 등)으로 나오며, 이상적으로 완벽히 일치해야 한다.
    
- **적대적 판별기**: 추가적인 신경망 구성 요소로, 입력된 이미지에 워터마크 메시지가 숨겨져 있는지 여부를 **이진 분류**하는 역할을 한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=convolutional%20networks%20for%20data%20hiding,We%20model%20this%20by). 인코더가 생성한 스테고 이미지가 이 판별기에 의해 **탐지되지 않도록** 인코더를 훈련시킴으로써, 워터마크가 **더 은밀하게(hidden)** 삽입되도록 유도한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=decoder%20network%20receives%20the%20encoded,We%20model%20this%20by). (판별기는 오직 훈련 단계에서만 활용되며, 추론 시에는 사용하지 않는다.)
    

HiDDeN의 이러한 구성은 **엔드투엔드 학습**으로 최적화된다. 인코더-디코더-노이즈(및 판별기)를 하나의 큰 모델처럼 묶어, **이미지 왜곡 최소화**(워터마크의 투명성)와 **메시지 복원 정확도 최대화**(워터마크의 견고성), 그리고 **워터마크 탐지 회피**라는 목적을 동시에 달성하도록 손실 함수를 설계하였다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Encoded%20images%20should%20look%20visually,of%20input%20messages%20and%20images). 구체적으로, 인코더와 디코더는 다음의 **합성 손실 함수**를 최소화하도록 학습된다:

LED=LI+α⋅LM+β⋅Ladv

여기서 $\mathcal{L}_{I}$는 원본 이미지와 삽입 이미지 간 차이를 나타내는 **이미지 손실**, $\mathcal{L}_{M}$는 원본 메시지와 복원 메시지의 차이를 나타내는 **메시지 손실**, $\mathcal{L}_{adv}$는 판별기를 속이는 정도를 나타내는 **적대적 손실**이며, $\alpha,\beta$는 각 손실 항목의 가중치 하이퍼파라미터이다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Encoded%20images%20should%20look%20visually,of%20input%20messages%20and%20images). 동시에 판별기는 실제 커버 이미지와 인코더가 만든 스테고 이미지를 구분하도록 학습(이진 분류 손실)되며, 인코더-디코더는 이 판별기를 최대한 속이도록(=$\mathcal{L}_{adv}$ 최소화) 훈련된다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=,of%20input%20messages%20and%20images). 요약하면, HiDDeN 논문의 핵심은 **작은 픽셀 변화로도 딥러닝을 통해 많은 비트를 은닉할 수 있음**을 보이고, **다양한 이미지 변형에도 강인한(blind & robust) 워터마크**를 심도록 **인코더-디코더-노이즈** 모델을 공동 최적화한 것이다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=produces%20a%20visually%20indistinguishable%20encoded,visual%20quality%20of%20encoded%20images).

## 2. 논문 구현 과정 요약: PyTorch를 이용한 구조 설계 및 학습 방식

HiDDeN의 모델 구조는 PyTorch 등 딥러닝 프레임워크로 비교적 간단히 구현할 수 있다. **인코더 신경망**과 **디코더 신경망**은 둘 다 합성곱 기반의 CNN 구조를 채택하며, 입력과 출력 형태에 따라 차이가 있다. 먼저 **인코더**는 원본 이미지(예: 3채널 RGB)와 숨길 메시지를 받아들이는데, 논문 구현에서는 **메시지**를 고정 길이(Bit)의 이진 벡터로 표현한다. 이 이진 메시지를 인코더에 직접 주입하기 위해, **메시지 비트를 공간 차원으로 복제(replication)하여 이미지와 동일한 크기의 채널 맵으로 확장**한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=A%20diagram%20for%20our%20system,The). 예를 들어 메시지 길이가 $k$비트라면, 크기가 $H\times W$인 $k$채널 짜리 **메시지 볼륨**으로 변환하는 식이다. 그런 다음 인코더는 원본 이미지에 몇 개의 합성곱 레이어를 적용해 **중간 특징 맵**을 얻고, 여기에 앞서 만든 메시지 볼륨을 **채널 방향으로 concatenate**하여 결합한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=intermediate%20representation,altering%20noise%20like%20cropping). 이를 통해 이후의 합성곱 계층들은 필터 연산 시 이미지의 모든 위치에서 전체 메시지 정보를 활용할 수 있어, 메시지 비트들을 이미지 전역에 고루 퍼뜨려 심을 수 있다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=length%20,convolutional%20layers%20to%20produce%20feature). 마지막으로 추가 합성곱층들을 거쳐 출력 채널을 3채널(RGB)로 줄이면 **스테고 이미지**가 생성된다. (합성곱 계층의 활성화 함수로 ReLU 등을 사용하며, 출력 이미지는 보통 원본과 동일한 $H\times W$ 크기로 얻도록 설계한다.)

**디코더**는 인코더의 역과정으로, **이미지에서 메시지를 추출**하는 합성곱 신경망이다. 디코더는 입력 이미지(노이즈로 왜곡된 워터마크 이미지를 포함)를 몇 층의 CNN으로 처리하여 **특징 맵**을 추출한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=The%20decoder%20first%20applies%20several,outputs%20a%20binary%20classification%20instead). 그 후 **Global Average Pooling**을 적용해 공간 차원을 평균화로 축약함으로써, 입력 이미지의 크기가 달라져도 고정 길이의 특징 벡터를 얻도록 한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=The%20decoder%20first%20applies%20several,outputs%20a%20binary%20classification%20instead). 마지막으로 **완전연결층(선형층)**을 통해 원하는 길이 $k$의 출력 벡터를 생성하며, 이것을 확률 또는 이진값으로 해석하여 복원된 메시지로 삼는다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=The%20decoder%20first%20applies%20several,outputs%20a%20binary%20classification%20instead). (예를 들어 각 비트에 시그모이드 함수를 적용하여 0/1 확률로 간주하거나 임계값을 넘어서는지 판단하여 이진 판정함.) **적대적 판별기**는 구조상 디코더와 유사하게 합성곱과 풀링으로 특징을 뽑은 뒤, 마지막에 시그모이드 등을 통해 **이미지에 워터마크 존재 여부**(예/아니오)를 출력하도록 구성한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=average%20pooling%20ensures%20that%20it,outputs%20a%20binary%20classification%20instead). 이 판별기는 진짜 이미지와 인코더 출력 이미지를 학습 데이터로 삼아 이진 분류기를 학습하며, 앞서 언급한 대로 인코더는 이 판별기를 속이도록 학습된다.

**노이즈 계층**은 구현상 인코더와 디코더 사이에 끼워넣는 **딥러닝 연산 모듈**이다. 논문에서는 여러 종류의 노이즈를 미분 가능하게近似 구현하여 학습에 사용했는데, PyTorch로 이러한 계층을 만드는 방법은 각 노이즈 종류별로 커스텀 연산을 정의하는 것이다. 대표적인 **노이즈(layer)** 종류는 다음과 같으며, 구현 시 확률적인 Tensor 연산이나 고정된 마스킹 연산 등으로 만들 수 있다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=The%20Identity%20layer%20is%20the,the%20distortion%3A%20Dropout%2C%20Cropout%2C%20and):

- **Identity**: 입력 이미지를 그대로 출력 (아무 변화 없음).
    
- **Dropout**: 일정 확률로 픽셀을 **원본 이미지 값으로 되돌리는** 노이즈. 예를 들어 드롭아웃 30%라면, 인코더 출력 이미지의 각 픽셀을 30% 확률로 원본 이미지의 해당 픽셀로 교체하고 나머지는 그대로 둔다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=layers%20undo%20some%20of%20the,the%20distortion%3A%20Dropout%2C%20Cropout%2C%20and). (이는 삽입한 비트의 일부를 무작위로 지워버리는 효과에 해당함.)
    
- **Cropout**: 인코더 출력의 **일부 영역**만 남기고 나머지는 원본으로 되돌리는 노이즈. 예를 들어 50% 크롭아웃은 무작위 위치의 정사각형 패치 한 군데는 인코더 출력을 유지하고, 그 밖의 영역 픽셀은 모두 원본으로 치환한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=layers%20undo%20some%20of%20the,the%20distortion%3A%20Dropout%2C%20Cropout%2C%20and).
    
- **Gaussian Blur**: 인코더 출력 이미지에 **가우시안 필터**를 적용하여 블러링(흐리게) 처리한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=from%20,the%20distortion%3A%20Dropout%2C%20Cropout%2C%20and).
    
- **Crop**: 이미지의 일부분만 **크롭(자르기)** 하고 다른 부분은 버리는 왜곡이다. 예를 들어 0.7 크롭이면 입력 이미지의 가로세로를 70% 크기로 무작위 자르고, 남은 부분은 잘린 것으로 처리하여 디코더에 공급한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=from%20,has%20a%20quality%20factor). (디코더는 이 경우 다양한 입력 크기에 대응할 수 있도록 전역 풀링을 사용한다.)
    
- **JPEG Compression**: 손실 JPEG 압축을 시뮬레이션하는 노이즈이다. JPEG의 비미분적 특성(양자화) 때문에, 학습 시에는 이를 근사하기 위해 **고주파 성분 마스킹**이나 **드롭아웃**을 DCT 도메인에서 적용하는 두 가지 버전을 사용하였다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=where%20the%20ratio%20of%20image,has%20a%20quality%20factor). 예컨대 **JPEG-Mask**는 DCT 변환 후 Y 채널 저주파 25개, U/V 채널 각 9개 계수만 남기고 나머지 고주파 계수를 0으로 마스킹하는 방식이고, **JPEG-Drop**은 실제 JPEG 양자화에서 많이 손실되는 고주파일수록 일정 확률로 드롭아웃하는 방식이다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,frequency%20coefficients.%20Models%20trained%20against)[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=We%20call%20the%20corresponding%20layers,actual%20JPEG%20compression%2C%20see%20Figure%C2%A05). 학습 단계에서 이러한 근사 JPEG 노이즈로 훈련하면, 실제 JPEG 압축에도 강인한 워터마크가 얻어지는 것으로 보고되었다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption)[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=is%20zeroed%20in%20our%20simulation,actual%20JPEG%20compression%2C%20see%20Figure%C2%A05).
    

以上과 같은 인코더, 디코더, 노이즈 모듈을 PyTorch로 구현한 후, **학습(training)**을 수행한다. 논문 구현에서는 **MS COCO 데이터셋**의 10,000장 이미지를 학습에 사용하고 1,000장을 테스트에 활용하였다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=All%20models%20are%20trained%20on,trained%20on%20multiple%20noise%20layers). 각 훈련 iteration마다 임의의 비트열을 생성하여 메시지로 삼고, 해당 메시지를 이미지에 은닉하도록 인코더-디코더를 학습시킨다. 최적화 알고리즘은 **Adam**(학습률 0.001 수준, PyTorch 기본 하이퍼파라미터)으로 설정하고, 배치 크기 12로 최대 200 에포크(epoch)까지 훈련을 진행하였다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=1000%20image%20test%20set%20unseen,trained%20on%20multiple%20noise%20layers). (단, 한 번에 여러 종류의 노이즈를 섞어 훈련하는 **Combined noise** 모델의 경우 수렴이 더디어 400 에포크까지 돌렸다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=drawn%20uniformly%20at%20random,trained%20on%20multiple%20noise%20layers).) 학습 중에는 앞서 설명한 합성 손실 $\mathcal{L}_{ED}$를 최소화하도록 인코더와 디코더 파라미터를 업데이트하고, 판별기는 $\mathcal{L}_{adv}$를 최대화(=자체 분류 정확도 극대화)하도록 별도로 훈련한다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=,of%20input%20messages%20and%20images). 이처럼 **교대로(인코더-디코더 vs 판별기)** 학습하는 **적대적 학습(Adversarial Training)** 전략을 통해, 최종적으로 인코더가 만든 이미지에는 판별기가 특별한 통계적 흔적을 찾지 못하면서도 디코더는 숨겨둔 정보를 정확히 복원할 수 있게 된다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=We%20introduce%20HiDDeN%2C%20the%20first,We%20model%20this%20by). 실제로 HiDDeN 모델은 동급의 기존 스테가노그래피 기법들과 **동등 이상의 용량과 은닉 성능**을 내면서, **가우시안 블러링, 픽셀 드롭아웃/크롭아웃, JPEG 압축** 등 다양한 **노이즈 공격에 견고한 워터마크**를 학습하는데 성공하였다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=produces%20a%20visually%20indistinguishable%20encoded,visual%20quality%20of%20encoded%20images). 예를 들어, 단일 모델을 **Dropout, Crop, Gaussian, JPEG** 등의 노이즈에 대해 함께 훈련시켰더니 각 개별 노이즈에 특화된 모델과 대등한 복원 성능을 보였고[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=noise%20layers,the%20visual%20quality%20of%20the), 심지어 이미지의 일부분(약 3.5%)만 남기고 잘라내는 극단적인 크롭 변형 후에도 높은 정확도로 워터마크를 복원해낼 수 있음을 보고하였다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Figure%203%3A%20Illustration%20of%20non,see%20Section%C2%A04). 이러한 결과를 통해 딥러닝 기반 워터마킹의 효과와 실용 가능성을 입증하였다.

## 3. 모델 경량화 과정: MobileNetV2로의 변환과 성능 변화

HiDDeN 모델의 기본 구현은 비교적 작은 합성곱 신경망들로 이루어져 있지만, **모바일 디바이스에서 실시간 추론**을 하기 위해서는 추가적인 **경량화**가 필요하다. 일반적으로 스마트폰 등의 환경에서는 연산 자원과 메모리가 제약되므로, 모델의 파라미터 수와 연산량(FLOPs)을 줄여 **추론 속도**를 높이고 **배터리 소모**를 줄여야 한다. 본 프로젝트에서는 이를 위해 **MobileNetV2** 기반으로 인코더/디코더를 경량화하였다. MobileNetV2는 구글에서 제안된 소형 CNN 모델로서, **깊이별 분리 합성곱(depthwise separable convolution)**과 **반전 잔차 블록(inverted residual block)** 등을 활용해 경량화와 정확도를 모두 달성한 모델이다[researchgate.net](https://www.researchgate.net/figure/Example-inverted-residual-block-using-depthwise-separable-convolutions-adapted-from-Li_fig2_393415445#:~:text=,6M%20parameters%20for%20edge). 깊이별 분리 합성곱이란 일반 합성곱을 두 단계(채널별 깊이 방향 합성곱 + 점별 1x1 합성곱)로 분해하여 연산량을 크게 줄이는 기법이며, MobileNetV2에서는 여기에 **선형 bottleneck** 구조를 결합한 inverted residual 블록을 사용한다[patrick-llgc.github.io](https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/mobilenets_v2.html#:~:text=llgc,memory%20requirement%20compared%20to)[researchgate.net](https://www.researchgate.net/figure/Example-inverted-residual-block-using-depthwise-separable-convolutions-adapted-from-Li_fig2_393415445#:~:text=,). 이러한 블록은 채널 수를 줄이는 1x1 합성곱으로 시작해 깊이별 합성곱으로 공간 처리를 하고, 다시 채널을 확장하는 1x1 합성곱으로 끝난 다음 입력과 출력을 스킵 연결(skip connection)하는 형태이다[researchgate.net](https://www.researchgate.net/figure/Example-inverted-residual-block-using-depthwise-separable-convolutions-adapted-from-Li_fig2_393415445#:~:text=,). 이를 통해 **특징 추출 능력을 유지하면서도 매개변수 수와 곱셈 연산량을 크게 감소**시킬 수 있다[researchgate.net](https://www.researchgate.net/figure/Example-inverted-residual-block-using-depthwise-separable-convolutions-adapted-from-Li_fig2_393415445#:~:text=,6M%20parameters%20for%20edge). 예를 들어 MobileNet 계열 모델은 수백만 개 수준의 소량의 파라미터만으로도 일반 CNN에 필적하는 정확도를 보이며, 실시간 처리에 적합한 효율을 달성한다[researchgate.net](https://www.researchgate.net/figure/Example-inverted-residual-block-using-depthwise-separable-convolutions-adapted-from-Li_fig2_393415445#:~:text=,6M%20parameters%20for%20edge).

우리의 워터마킹 인코더/디코더를 MobileNetV2로 경량화하기 위해, **기존의 합성곱 층들을 MobileNetV2의 블록들로 교체**하거나 **유사한 구조로 재설계**하였다. 구체적으로는 다음과 같은 전략을 적용하였다:

- 인코더의 경우, 처음 부분의 몇 개 합성곱을 MobileNetV2의 **인버티드 residual 블록**으로 대체하였다. 예를 들어, 입력 이미지를 처리하는 첫 합성곱 대신 채널을 확장하는 1x1 컨볼루션 -> 3x3 depthwise 컨볼루션 -> 1x1 축소 컨볼루션 + residual 연결 구조로 변경하였다. 이렇게 하면 동일한 특성 맵 크기를 얻으면서도 연산량이 줄어든다. 또한 메시지 볼륨을 결합한 이후의 합성곱들도 일부 채널을 줄이고 depthwise 연산을 활용하는 식으로 수정하여 전체 파라미터수를 감축했다.
    
- 디코더의 경우에도 표준 합성곱 대신 3x3 depthwise + 1x1 conv 조합을 사용한 블록으로 구성하였다. 디코더는 여러 층에 걸쳐 다운샘플링(풀링)과 특징 추출을 수행하므로, MobileNetV2 스타일의 stride가 있는 블록들을 넣어 점진적으로 해상도를 줄였다. 마지막 복원 단계 직전의 합성곱도 1x1 축소-확장 구조로 만들어 필요 최소한의 채널로 메시지 벡터를 뽑도록 하였다.
    

이러한 변경으로 **모델의 규모(파라미터 수)**와 **연산 복잡도**가 크게 줄었다. 실제 경량화된 모델은 원본 HiDDeN 모델 대비 파라미터가 수분의 일 수준으로 감소하였다. 예를 들어, 원래 디코더가 수십만 개의 매개변수를 가졌다면 MobileNetV2 기반 디코더는 수만 개 이하로 줄어드는 식이다. 이에 따라 **모델 크기**(메모리 사용량)도 작아지고 추론시 **연산 지연(latency)**도 눈에 띄게 개선된다. 특히 MobileNetV2 특유의 효율 덕분에 정확도 저하를 최소화하면서 경량화를 달성했는데, 일반적으로 깊이별 분리 합성곱 사용으로 인한 정확도 손실은 매우 작다는 것이 알려져 있다[researchgate.net](https://www.researchgate.net/figure/Example-inverted-residual-block-using-depthwise-separable-convolutions-adapted-from-Li_fig2_393415445#:~:text=,6M%20parameters%20for%20edge). 우리의 워터마크 실험에서도 경량화 후 **메시지 복원 정확도**가 약간 떨어지긴 했지만(예: 복원 비트 정확도 98% -> 96% 수준), 여전히 높은 수준을 유지하였다. 반면 **처리 속도**는 크게 향상되어, 중급 스마트폰 CPU에서도 한 장의 이미지에 대한 워터마크 삽입 또는 추출을 **수십 밀리초 이내**에 수행할 수 있었다. 이는 **실시간**으로 사용자 인터페이스를 제공하는 데 무리가 없는 속도이다.

추가로, 우리는 **양자화(Quantization)** 기법도 적용하여 모델을 최적화하였다. PyTorch는 **포스트 트레이닝 양자화** 등을 지원하며, 32-bit 부동소수점 가중치를 8-bit 정수로 변환하면 모델 용량이 1/4 수준으로 줄고 CPU 연산속도가 향상된다[heartbeat.comet.ml](https://heartbeat.comet.ml/pytorch-mobile-exploring-facebooks-new-mobile-machine-learning-solution-96c99efbfd58?gi=33a5ab8dbede#:~:text=,needing%20to%20work%20through%20other). 양자화된 워터마킹 모델을 만들고 나니, 거의 성능 저하 없이도 모델 크기를 크게 감축할 수 있었다. 이처럼 MobileNetV2 기반 구조 변경과 양자화를 통해 **모바일 앱에 적합한 경량 워터마킹 모델**을 확보하였다. 경량화된 최종 모델은 원본 HiDDeN의 워터마크 강인성을 대부분 유지하면서도, 모바일 환경에서 **실시간 임베딩/추출**이 가능한 수준의 속도를 달성하였다.

## 4. SNS 이미지 보호 시나리오 예시

이 절에서는 구현한 워터마킹 시스템이 실제 **소셜 미디어(SNS)** 이미지 공유 환경에서 어떻게 활용될 수 있는지를 예시 시나리오로 설명한다. 가정하기를, 한 대학생(Alice)이 자신의 사진 혹은 디지털 아트워크에 보이지 않는 워터마크를 삽입한 뒤 인스타그램과 같은 SNS에 게시한다고 하자. Alice는 미리 준비된 모바일 앱을 사용하여 이미지에 자신의 고유 ID를 워터마크로 넣는다. SNS에 업로드된 이미지는 플랫폼에 의해 자동으로 용량이 줄어들도록 JPEG 압축이 이루어지고, 경우에 따라 리사이즈나 포맷 변환 등의 처리가 가해질 수 있다. 시간이 지난 후 만약 타인이 해당 이미지를 무단으로 사용하거나 저작권 분쟁이 발생했을 때, Alice(또는 검증 당국)은 **워터마크 탐지** 도구를 이용해 온라인에 떠도는 이미지에서 자신의 ID를 복원하고 이를 증거로 제시할 수 있다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=steganography%2C%20the%20goal%20is%20secret,by%20embedding%20digital%20watermarks%20in). 일반적인 디지털 워터마킹 활용 사례와 마찬가지로, 우리의 딥러닝 워터마크도 **저작권 보호**나 **출처 추적**에 활용될 수 있는 것이다.

_그림 2: SNS 상의 이미지에 대한 워터마크 삽입 및 검출 흐름도. (1) 사용자가 원본 이미지를 준비하고 자신의 식별자(ID)를 입력하여 인코더를 통해 보이지 않는 워터마크가 삽입된 이미지를 생성한다. (2) 이 **워터마크 이미지**를 SNS에 게시하면, 업로드 과정에서 JPEG 압축 등으로 화질 열화나 노이즈가 추가될 수 있다. (3) SNS에서 내려받은 이미지(혹은 스크린샷 등)에 대하여 디코더를 적용하면, 삽입됐던 비밀 ID를 복원할 수 있다. 이를 통해 설령 이미지가 변형되거나 복사되었더라도 원 소유자를 확인하거나 **불법 유포를 추적**할 수 있다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=steganography%2C%20the%20goal%20is%20secret,by%20embedding%20digital%20watermarks%20in)._

위의 흐름을 정리하면 다음과 같다:

1. **워터마크 삽입**: 사용자는 모바일 앱에서 워터마킹 기능을 실행한다. 워터마크로 삽입할 **텍스트 ID**를 입력하고, 워터마크를 넣을 **원본 이미지를 선택**한다. 앱 내의 인코더 모델이 해당 이미지 픽셀들에 ID를 암호화하여 삽입한 **워터마크 이미지를 출력**한다. 이 이미지는 눈으로 볼 때 원본과 차이가 없으며, ID는 내부에 숨겨진 상태다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Image%3A%20Refer%20to%20caption%20Figure,high%20accuracy%20by%20the%20decoder).
    
2. **SNS 게시 및 전송**: 사용자는 이 워터마크 이미지를 SNS에 업로드한다. SNS 서버는 이미지를 저장하면서 **JPEG 압축**이나 리사이즈 등 처리를 수행하므로, 워터마크 이미지에 약간의 손상이 가해진다. 예를 들어 해상도가 낮아지거나 압축 아티팩트가 생길 수 있다. 하지만 우리 모델은 이런 **노이즈 환경에서도 견딜 수 있도록 학습**되었기 때문에, 적당한 수준의 압축이나 필터링에서는 워터마크가 망가지지 않는다[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=existing%20data%20hiding%20algorithms%2C%20and,visual%20quality%20of%20encoded%20images).
    
3. **워터마크 검출**: 시간이 지나 해당 이미지의 **출처를 확인**해야 하는 상황이 생기면, 관계자는 유통 중인 이미지를 입수하여 디코더에 입력한다. 디코더 모델은 손상된 이미지에서도 숨겨진 ID를 **복원**해낸다. 복원된 ID가 Alice의 것과 일치하면, 그 이미지가 Alice가 원 게시자임을 증명할 수 있다. 특히 우리의 워터마크는 **블러, 노이즈 추가, 크롭, 압축** 등의 공격 후에도 높은 정확도로 ID를 찾아낼 수 있으므로[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=produces%20a%20visually%20indistinguishable%20encoded,visual%20quality%20of%20encoded%20images)[ar5iv.org](https://ar5iv.org/abs/1807.09937#:~:text=Figure%203%3A%20Illustration%20of%20non,see%20Section%C2%A04), SNS상에서 일반적으로 일어나는 이미지 변형에 대해 강인한 인증 수단이 된다.
    

이 시나리오에서 볼 수 있듯, 딥러닝 기반의 워터마킹 시스템은 **사용자 친화적인 모바일 앱** 형태로 제공되어 쉽게 활용될 수 있다. 예를 들어 콘텐츠 제작자는 이미지를 올릴 때 자동으로 자신의 워터마크를 심을 수 있고, 별도의 검사 모드에서 이미지에 숨겨진 워터마크를 읽어낼 수도 있다. 중요한 것은 이러한 워터마크가 **보호 효용성**과 **은폐성** 사이에서 균형을 이룬다는 점이다. 눈에 보이지 않으면서도 저작권을 주장할 때는 증거로 활용할 수 있고, SNS 사업자들이 불법 복제를 모니터링하는 데에도 활용 가능하다. 이처럼 **HiDDeN 기반 워터마킹**은 SNS 시대의 디지털 콘텐츠 보호에 새로운 가능성을 열어준다.

## 5. Android/iOS 앱 연동 방안

마지막으로, 구현된 워터마킹 모델을 실제 **모바일 앱**과 연동하는 방안을 간략히 소개한다. 우리의 목표는 Android와 iOS 스마트폰에서 모두 구동 가능한 앱을 제작하여, 사용자가 실시간으로 이미지를 워터마킹하거나 워터마크를 검출할 수 있도록 하는 것이다. 이를 위해 고려한 요소는 **모델 파일 배포**, **플랫폼별 최적화**, **실시간 추론을 위한 엔진 통합** 세 가지이다.

**(1) 모델 변환 및 배포:** 딥러닝 모델을 모바일에서 실행하려면 일반적으로 모델을 특정 형식으로 변환해야 한다. 우리는 PyTorch로 학습한 모델을 **TorchScript** 형식으로 변환하여 사용하였다. TorchScript는 PyTorch 모델을 직렬화하여 C++ 런타임에서 실행할 수 있게 해주는 포맷으로, PyTorch 1.3부터 제공된 **PyTorch Mobile** 기능을 통해 iOS/Android에서 직접 로드하여 추론할 수 있다[heartbeat.comet.ml](https://heartbeat.comet.ml/pytorch-mobile-exploring-facebooks-new-mobile-machine-learning-solution-96c99efbfd58?gi=33a5ab8dbede#:~:text=Put%20simply%2C%20PyTorch%20Mobile%20is,inside%20iOS%20and%20Android%20applications). 변환된 `.pt` 모델 파일(양자화 적용 시 수 MB 이하)은 앱 패키지에 포함하거나 필요 시 서버에서 다운로드하도록 구성한다. Android 앱의 경우 PyTorch Android API를 이용해 모델을 로드하고 Java/Kotlin에서 Tensor를 만들어 추론을 호출하며, iOS의 경우 Objective-C++ 브리지를 통해 TorchScript 모델을 실행하거나, 필요하면 Core ML용으로 변환하는 방안도 고려된다. (예: ONNX로 내보낸 뒤 Core ML Tools로 변환하면 iOS의 Neural Engine 가속을 활용할 수도 있다.)

**(2) 플랫폼별 최적화:** Android에서는 PyTorch Mobile 외에도 **TensorFlow Lite**를 사용할 수도 있다. 예를 들어 TorchScript 대신 ONNX로 모델을 내보내어 TFLite 모델로 변환하면, Android Neural Networks API(NNAPI)를 통해 하드웨어 가속을 받을 수 있다. 그러나 PyTorch Mobile만으로도 CPU 기반 실시간 처리가 가능하므로 우선 TorchScript 경로를 채택했다. iOS에서는 **Core ML**이 Apple의 가속 프레임워크인데, 앞서 언급한 대로 필요하면 모델을 CoreML(.mlmodel)로 변환할 수 있다. 다만 PyTorch Mobile을 쓸 경우에도 GPU를 활용하지는 않지만 충분한 속도가 나오므로, 앱 개발 편의를 위해 TorchScript 방식을 유지했다. **Quantization** 기법은 이미 모델에 적용되었으므로, 모바일에서 모델 로드 시 8-bit 연산이 자동 활용되어 속도가 향상된다[heartbeat.comet.ml](https://heartbeat.comet.ml/pytorch-mobile-exploring-facebooks-new-mobile-machine-learning-solution-96c99efbfd58?gi=33a5ab8dbede#:~:text=,needing%20to%20work%20through%20other). 또한 PyTorch Mobile은 필요한 연산자만 포함하는 경량 런타임으로 패키징할 수 있어 앱 크기를 최적화하였다.

**(3) 애플리케이션 통합:** 앱 UI 측면에서는, 사용자가 **갤러리 사진을 선택하거나 카메라로 촬영**한 이미지를 입력으로 받아들이고, **워터마크 삽입** 또는 **추출** 버튼을 누르면 백그라운드에서 TorchScript 모델을 불러 추론을 수행하도록 구현한다. 예를 들어 워터마크 삽입의 경우, 선택된 이미지와 사용자가 입력한 ID 문자열을 앱이 받아서, ID를 이진 메시지로 변환 후 TorchScript 인코더를 호출한다. 결과로 얻은 워터마크 이미지를 화면에 미리보기로 보여주고 기기 저장소에 저장하거나 바로 SNS 공유할 수 있게 한다. 추출 기능의 경우도, 선택된 이미지(예: 다운로드한 의심 이미지)를 디코더 모델에 입력하여 복원된 ID를 문자열로 보여준다. 이러한 동작은 모두 **온-디바이스**에서 수행되므로 인터넷 연결 없이도 동작하며, 지연 시간도 짧아서 사용자가 거의 즉각적인 결과를 얻을 수 있다.

정리하면, PyTorch로 구현된 딥러닝 워터마킹 모델을 **TorchScript 변환** 및 **PyTorch Mobile 통합**을 통해 Android/iOS 앱에 삽입하였다. 이 앱은 이미지 콘텐츠가 SNS에 게시되기 전에 **워터마크를 삽입**하고, 필요시 게시된 콘텐츠에서 **워터마크를 추출**하는 기능을 제공한다. 딥러닝 모델의 특성상 지속적인 **재학습**이나 **업데이트**도 가능하므로, 예를 들어 새로운 노이즈 유형이 등장하면 서버 측에서 모델을 재훈련해 배포함으로써 앱의 워터마크 복원 능력을 향상시킬 수도 있다. 제안된 시스템은 사용자 친화적인 인터페이스와 모바일 최적화를 바탕으로, **대용량의 이미지 콘텐츠가 빠르게 유통되는 SNS 환경에서 실시간으로 적용 가능한 워터마킹 솔루션**을 제공한다. 이는 콘텐츠 제작자와 플랫폼이 **디지털 저작권을 보호**하고 **위변조를 추적**하는 데 유용한 도구가 될 것으로 기대된다.