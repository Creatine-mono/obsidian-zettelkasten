**인공지능이란**
1. 인공지능의 시대
생성형 AI의 진화 : ChatGPT, DeepSeek, Gemini
영상 추천시스템 : Netflix, YouTube

**인공지능 역사**
인공지능 기술은 인간의 지능을 기계 등에 인공적으로 구현하려는 기술로, 부흥기 / 침체기를 반복해왔다.

![[Pasted image 20250722101909.png]]

**XOR문제**

XOR 연산은 선형적으로 분리 불가능한 문제이므로, 단층 퍼셉트론으로는 해결할 수 없습니다. 따라서 XOR 문제를 해결하려면 다층 퍼셉트론(Multi-Layer Perceptron, MLP)을 사용해야 합니다

**기울기 소실 문제 (Vanishing Gradient Problem)**

1.  **정의**: 심층 신경망(Deep Neural Network)에서 역전파 과정 중 하위 레이어로 갈수록 기울기가 점차 작아져 가중치 업데이트가 제대로 이루어지지 않는 현상.
2.  **원인**:
    *   **활성화 함수**: Sigmoid나 Tanh와 같은 활성화 함수는 입력값이 특정 범위에서 벗어나면 기울기가 0에 가까워짐.
    *   **심층 네트워크**: 네트워크가 깊어질수록 기울기가 여러 층을 거치면서 곱해지기 때문에, 작은 기울기가 반복적으로 곱해지면 기울기가 소실됨.
3.  **영향**:
    *   **학습 지연**: 하위 레이어의 가중치가 업데이트되지 않아 학습이 느려지거나 멈춤.
    *   **성능 저하**: 모델이 제대로 학습되지 않아 전체적인 성능이 저하됨.
4.  **해결 방법**:
    *   **ReLU 활성화 함수**: ReLU는 양수 영역에서 기울기가 1이므로 기울기 소실을 완화.
    *   **Leaky ReLU, Parametric ReLU**: ReLU의 변형으로, 입력이 음수일 때도 작은 기울기를 가짐.
    *   **Batch Normalization**: 각 레이어의 입력을 정규화하여 기울기가 너무 작거나 커지는 것을 방지.
    *   **Residual Connection (ResNet)**: 이전 레이어의 출력을 현재 레이어의 출력에 더하여 기울기가 직접 전달되도록 함.
    *   **Gradient Clipping**: 기울기가 특정 값 이상으로 커지는 것을 방지.
생성형 인공지능은 대규모 언어모델(LLM)을 기반으로 텍스트, 이미지, 코드등 다양한 형태의 콘텐츠를 자동으로 생성하는 기술이다.
### ChatGPT

*   **개발사**: OpenAI
*   **모델 아키텍처**: Transformer 기반
*   **학습 데이터**: 대량의 텍스트 및 코드 데이터
*   **성능**: 자연스러운 텍스트 생성, 다양한 자연어 처리 작업 수행

### Gemini

*   **개발사**: Google
*   **모델 아키텍처**: Transformer 기반 (멀티모달)
*   **학습 데이터**: 텍스트, 이미지, 오디오, 비디오 등 다양한 데이터
*   **성능**: 텍스트, 이미지, 오디오, 비디오 등 다양한 데이터 이해 및 생성

### DeepSeek

*   **개발사**: 중국 AI 기업
*   **모델 아키텍처**: Transformer 기반
*   **학습 데이터**: 대량의 코드 데이터
*   **성능**: 코드 생성 및 이해 능력에 특화

