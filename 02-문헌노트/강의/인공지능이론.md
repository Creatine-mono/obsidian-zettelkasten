박정규 멘토

## 정보처리기사

### 응시자격

*   학력, 경력 제한 없음

### 시험 과목

1.  소프트웨어 설계
2.  소프트웨어 개발
3.  데이터베이스 구축
4.  프로그래밍 언어 활용
5.  정보시스템 구축 관리

### 합격 기준

*   필기: 100점 만점에 과목당 40점 이상, 전 과목 평균 60점 이상
*   실기: 100점 만점에 60점 이상

### 응시료

*   필기: 19,400원
*   실기: 22,600원

## ISMS-P

### 인증 범위

*   정보보호 관리체계
*   개인정보보호 관리체계

### 인증 기준

1.  관리체계 수립 및 운영
2.  위험 관리
3.  정보보호 대책
4.  개인정보보호 대책
5.  지속적 개선

### 인증 수수료

*   신규 인증: 3,000,000원 ~ 20,000,000원 (기업 규모에 따라 상이)
*   갱신 인증: 신규 인증 수수료의 70%

http://url.kr/jp2kyo

인공지능 
사람이 해야할일을 기계가 대신할 수 있는 모든 자동화에 해당
머신러닝 
명시적으로 규칙을 프로그래밍하지 않고 데이터로부터 의사결정을 위한 패턴을 기계가 스스로 학습
딥러닝
인공신경망 기반의 모델로, 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한번에 수행

데이터/현상 --특징 추출--> 특징 (frature) --판단--> 인식 및 예측

사람 사람 
문제해결에 필요한 주요 정보와 판단규칙을 사람이 설계
예: 계산기처럼 사람의 연산 방식을 구현하는 규칙 기반 프로그래밍

사람 Machine
문제 해결에 필요한 주요 특징만 사람이 결정
*   **머신러닝:** 알고리즘이 데이터에서 학습하여 패턴을 식별하고 예측을 수행합니다. 특징 추출은 여전히 사람이 설계하지만, 모델은 데이터에 따라 스스로 개선됩니다.

Machine Machine
*   **딥러닝:** 인공신경망을 사용하여 특징 추출과 판단을 모두 기계가 자동으로 수행합니다. 사람이 직접 특징을 설계할 필요가 없으며, 모델은 대량의 데이터를 통해 복잡한 패턴을 학습합니다.

**모라벡의 역설**

모라벡의 역설은 고차원적인 추론 능력은 계산 능력을 별로 필요로 하지 않지만, 저차원적인 감각-운동 능력은 엄청난 계산 자원을 필요로 한다는 관찰입니다. 다시 말해, 인간에게 쉬운 것(예: 지각, 운동 능력)은 AI에게 어렵고, 인간에게 어려운 것(예: 복잡한 계산, 게임)은 AI에게 쉽다는 것입니다.

의사소통
youtube Cnn otavio Good

## IT (Information Technology)

IT(Information Technology)는 정보 기술을 의미하며, 컴퓨터, 소프트웨어, 네트워크, 데이터베이스, 정보 시스템 등 정보를 생성, 저장, 처리, 전송, 활용하는 데 필요한 모든 기술과 관련된 광범위한 분야를 포괄합니다. IT는 현대 사회의 거의 모든 측면에 영향을 미치며, 비즈니스, 정부, 교육, 의료, 엔터테인먼트 등 다양한 산업에서 핵심적인 역할을 수행합니다.

**IT 주요 분야:**

*   **컴퓨터 공학 (Computer Engineering):** 컴퓨터 하드웨어 및 소프트웨어 시스템 설계 및 개발
*   **소프트웨어 공학 (Software Engineering):** 소프트웨어 개발, 유지 보수 및 관리
*   **네트워크 (Networking):** 컴퓨터 네트워크 설계, 구축 및 관리
*   **데이터베이스 (Database):** 데이터베이스 설계, 구축 및 관리
*   **정보 보안 (Information Security):** 정보 시스템 및 데이터 보호

## 보안 (Security)

보안은 시스템, 네트워크, 데이터 등을 보호하여 **기밀성 (Confidentiality), 무결성 (Integrity), 가용성 (Availability)**을 유지하는 것을 목표로 합니다.

*   **기밀성 (Confidentiality):** 권한이 없는 사용자가 정보에 접근하는 것을 방지합니다. (예: 암호화, 접근 제어)
*   **무결성 (Integrity):** 정보가 정확하고 완전하게 유지되도록 보호합니다. (예: 데이터 유효성 검사, 변경 감지)
*   **가용성 (Availability):** 필요한 사용자가 언제든지 정보와 시스템에 접근할 수 있도록 보장합니다. (예: 시스템 이중화, 백업)

**보안 주요 분야:**

*   **사이버 보안 (Cybersecurity):** 컴퓨터 시스템, 네트워크 및 데이터를 사이버 공격으로부터 보호
*   **정보 보안 (Information Security):** 정보 자산의 기밀성, 무결성 및 가용성 유지
*   **물리적 보안 (Physical Security):** 물리적 자산 (예: 건물, 장비) 보호

## AI (Artificial Intelligence)

AI(Artificial Intelligence)는 인공지능을 의미하며, 기계가 인간의 지능을 모방하여 학습, 추론, 문제 해결, 의사 결정 등의 능력을 수행하는 기술입니다. AI는 머신러닝 (Machine Learning), 딥러닝 (Deep Learning), 자연어 처리 (Natural Language Processing), 컴퓨터 비전 (Computer Vision), 로보틱스 (Robotics) 등 다양한 하위 분야를 포함하며, 자율 주행 자동차, 의료 진단, 금융 분석, 고객 서비스, 스마트 홈 등 다양한 분야에서 활용되고 있습니다.

**AI 주요 분야:**

*   **머신러닝 (Machine Learning):** 데이터로부터 학습하여 패턴을 식별하고 예측을 수행하는 알고리즘 및 기술
*   **딥러닝 (Deep Learning):** 인공 신경망 (Artificial Neural Network)을 사용하여 복잡한 패턴을 학습하는 머신러닝의 한 분야
*   **자연어 처리 (Natural Language Processing):** 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 기술
*   **컴퓨터 비전 (Computer Vision):** 컴퓨터가 이미지와 비디오를 이해하고 분석할 수 있도록 하는 기술
*   **로보틱스 (Robotics):** 로봇을 설계, 제작, 작동 및 응용하는 기술

## AI 보안 (AI Security)

AI 보안은 인공지능 시스템을 보호하고, 인공지능 기술을 활용하여 보안을 강화하는 분야입니다. AI 시스템은 **적대적 공격 (Adversarial Attack), 데이터 오염 (Data Poisoning), 모델 유출 (Model Extraction)** 등 다양한 보안 위협에 노출될 수 있으며, AI 보안은 이러한 위협으로부터 AI 시스템을 보호하는 데 초점을 맞춥니다. 또한, AI 기술은 사이버 공격 탐지, 이상 징후 분석, 위협 예측, 자동화된 보안 대응 등 보안 분야에서 다양한 방식으로 활용될 수 있습니다.

**AI 보안 주요 분야:**

*   **적대적 방어 (Adversarial Defense):** 적대적 공격으로부터 AI 모델을 보호하는 기술
*   **AI 기반 보안 (AI-powered Security):** AI 기술을 활용하여 보안 시스템을 강화하는 기술
*   **AI 윤리 및 안전 (AI Ethics and Safety):** AI 시스템의 안전하고 윤리적인 사용을 보장하는 기술

## 오버피팅 (Overfitting)

오버피팅(Overfitting)은 머신러닝 모델이 학습 데이터에 너무 과도하게 맞춰져서, **새로운 데이터 (Unseen Data)에 대한 예측 성능이 떨어지는 현상**입니다. 모델이 학습 데이터의 노이즈 (Noise)까지 학습하여 일반화 능력 (Generalization Ability)을 잃게 되는 것입니다.

**오버피팅 해결 방법:**

*   **더 많은 데이터 확보 (More Data):** 학습 데이터의 양을 늘립니다.
*   **데이터 증강 (Data Augmentation):** 기존 데이터를 변형하여 새로운 데이터를 생성합니다.
*   **모델 복잡도 감소 (Reduce Model Complexity):** 모델의 파라미터 수를 줄이거나, 더 간단한 모델을 사용합니다.
*   **정규화 (Regularization):** 모델의 파라미터에 제약을 가하여 복잡도를 줄입니다. (예: L1 정규화, L2 정규화)
*   **드롭아웃 (Dropout):** 학습 과정에서 뉴런의 일부를 임의로 제거하여 오버피팅을 방지합니다.
*   **조기 종료 (Early Stopping):** 검증 데이터 (Validation Data)에 대한 성능이 더 이상 개선되지 않으면 학습을 중단합니다.

## Active Learning - Query Strategy

Active Learning에서 Query Strategy는 모델이 학습할 데이터를 선택하는 방법입니다. 목표는 **가장 유익한 데이터 (Most Informative Data)를 선택하여 학습 효율성을 극대화**하는 것입니다.

**주요 Query Strategy:**

*   **불확실성 샘플링 (Uncertainty Sampling):** 모델이 가장 불확실하게 예측하는 데이터 포인트를 선택합니다. (예: Least Confidence, Margin Sampling, Entropy Sampling)
*   **기대 모델 변화 (Expected Model Change):** 모델의 가장 큰 변화를 가져올 것으로 예상되는 데이터 포인트를 선택합니다.
*   **예상 오류 감소 (Expected Error Reduction):** 모델의 오류를 가장 많이 줄일 수 있는 데이터 포인트를 선택합니다.
*   **쿼리-바이-커미티 (Query-by-Committee):** 여러 모델 (Committee)을 사용하여 가장 불일치하는 데이터 포인트를 선택합니다.

## 원핫인코딩 (One-Hot Encoding)

원핫인코딩은 범주형 데이터 (Categorical Data)를 머신러닝 모델이 이해할 수 있는 **수치형 데이터 (Numerical Data)로 변환하는 방법** 중 하나입니다. 각 범주를 이진 벡터 (Binary Vector)로 표현하며, 각 벡터는 하나의 요소만 1이고 나머지는 0인 형태를 가집니다.

**예시:**

*   색상: "빨강", "초록", "파랑"
*   원핫인코딩:
    *   빨강: [1, 0, 0]
    *   초록: [0, 1, 0]
    *   파랑: [0, 0, 1]

## CBOW (Continuous Bag of Words)

CBOW는 Word2Vec 모델의 한 종류로, **주변 단어들 (Context Words)을 사용하여 중심 단어 (Target Word)를 예측**하는 방식입니다. CBOW는 단어의 분산 표현 (Distributed Representation) 또는 워드 임베딩 (Word Embedding)을 학습하는 데 사용됩니다.

**예시:**

*   문장: "The cat sat on the mat"
*   중심 단어: "sat"
*   주변 단어: "The", "cat", "on", "the", "mat"

## 포켓몬고 워드임베딩 (Pokemon GO Word Embedding)

포켓몬고 워드임베딩은 포켓몬고 게임과 관련된 텍스트 데이터를 사용하여 **단어의 의미를 벡터 공간에 표현하는 기술**입니다. 이를 통해 포켓몬 이름, 기술 이름, 아이템 이름 등의 단어 간의 유사성을 파악하거나, 게임 관련 텍스트 데이터를 분석하는 데 활용할 수 있습니다.

## MRC (Machine Reading Comprehension)
MRC(Machine Reading Comprehension)는 기계가 주어진 **지문 (Context)을 이해하고 질문 (Question)에 대한 정답 (Answer)을 찾아내는 task**입니다. MRC는 자연어 이해 (Natural Language Understanding) 능력을 평가하는 데 사용됩니다.

**MRC 주요 데이터셋:**

*   **SQuAD (Stanford Question Answering Dataset)**
*   **KorQuAD (Korean Question Answering Dataset)**
*   **RACE (ReAding Comprehension from Examinations)**

## AI Process

AI Process는 인공지능 모델을 개발하고 배포하는 전체 과정을 의미합니다.

**AI Process 주요 단계:**

1.  **문제 정의 (Problem Definition):** 해결하고자 하는 문제를 명확하게 정의합니다.
2.  **데이터 수집 (Data Collection):** 모델 학습에 필요한 데이터를 수집합니다.
3.  **데이터 전처리 (Data Preprocessing):** 데이터를 정리하고 모델에 적합한 형태로 변환합니다.
4.  **모델 선택 (Model Selection):** 문제에 적합한 AI 모델을 선택합니다.
5.  **모델 학습 (Model Training):** 데이터를 사용하여 모델을 학습시킵니다.
6.  **모델 평가 (Model Evaluation):** 학습된 모델의 성능을 평가합니다.
7.  **모델 배포 (Model Deployment):** 모델을 실제 환경에 배포합니다.
8.  **모델 모니터링 (Model Monitoring):** 모델의 성능을 지속적으로 모니터링하고 필요에 따라 재학습합니다.

## 조기 종료 (Early Stopping)

조기 종료(Early Stopping)는 머신러닝 모델의 학습 과정에서 **검증 데이터 (Validation Data)에 대한 성능이 더 이상 개선되지 않으면 학습을 중단하는 방법**입니다. 오버피팅을 방지하고 모델의 일반화 성능을 높이는 데 사용됩니다.

## 데이터 증강 (Data Augmentation)

데이터 증강(Data Augmentation)은 머신러닝 모델의 학습에 사용되는 데이터의 양을 늘리기 위해 **기존 데이터를 변형하거나 새로운 데이터를 생성하는 방법**입니다.

**주요 데이터 증강 기법:**

*   **이미지 데이터:** 회전 (Rotation), 이동 (Translation), 확대/축소 (Scaling), 좌우 반전 (Flipping), 색상 변경 (Color Jittering) 등
*   **텍스트 데이터:** 단어 대체 (Word Replacement), 문장 재구성 (Sentence Reordering), 역번역 (Back Translation) 등

## Capacity 줄이기 (Reducing Capacity)

모델 Capacity를 줄이는 것은 모델의 복잡도를 낮추는 것을 의미합니다. 이는 모델이 학습 데이터에 과도하게 적합되는 것을 방지하고 일반화 성능을 향상시키는 데 도움이 됩니다.

**모델 Capacity를 줄이는 방법:**

*   **레이어 수 줄이기 (Reducing Number of Layers):** 신경망의 레이어 수를 줄입니다.
*   **레이어 크기 줄이기 (Reducing Layer Size):** 각 레이어의 뉴런 수를 줄입니다.
*   **파라미터 수 줄이기 (Reducing Number of Parameters):** 모델의 전체 파라미터 수를 줄입니다.

## 드롭아웃 (Dropout)

드롭아웃(Dropout)은 신경망 모델에서 **오버피팅을 줄이기 위해 사용되는 정규화 기술** 중 하나입니다. 학습 과정에서 각 반복마다 신경망의 뉴런 중 일부를 임의로 비활성화함으로써, 모델이 특정 뉴런에 과도하게 의존하는 것을 방지합니다.

## 전이 학습 (Transfer Learning)

전이 학습(Transfer Learning)은 **한 task에서 학습한 지식을 다른 task에 적용하는 방법**입니다. 일반적으로 대규모 데이터셋으로 사전 학습된 모델 (Pre-trained Model)을 가져와서 특정 task에 맞게 Fine-tuning하는 방식으로 사용됩니다. 전이 학습은 데이터가 부족한 task에서 모델의 성능을 향상시키는 데 효과적입니다.

**전이 학습 주요 방법:**

*   **Fine-tuning:** Pre-trained Model의 모든 레이어를 재학습합니다.
*   **Feature Extraction:** Pre-trained Model의 일부 레이어를 고정하고, 나머지 레이어만 재학습합니다.

## 사전 학습된 AI (Pre-trained AI)

사전 학습된 AI (Pre-trained AI)는 **대량의 데이터로 미리 학습된 인공지능 모델**을 의미합니다. 이러한 모델은 다양한 task에 적용할 수 있도록 일반적인 특징을 학습한 상태이며, 특정 task에 맞게 Fine-tuning하여 사용할 수 있습니다. 사전 학습된 AI는 자연어 처리, 컴퓨터 비전 등 다양한 분야에서 활용되고 있습니다.

## BERT (Bidirectional Encoder Representations from Transformers)

BERT는 Google에서 개발한 **Transformer 기반의 사전 학습된 언어 모델**입니다. BERT는 양방향으로 문맥을 이해할 수 있으며, 다양한 자연어 처리 task (예: 질의 응답, 텍스트 분류, 개체명 인식)에서 높은 성능을 보입니다. BERT는 사전 학습된 모델을 Fine-tuning하는 전이 학습 방식으로 널리 사용되고 있습니다.


설명가능한 인공지능(XAI, eXplainable Artificial Intelligence)은 인공지능 모델의 의사 결정 과정과 결과를 사람이 이해할 수 있도록 설명하는 것을 목표로 하는 분야입니다. 기존의 블랙박스 모델과는 달리, XAI는 모델이 왜 특정 결정을 내렸는지, 어떤 요인이 중요한 영향을 미쳤는지 등을 설명할 수 있게 합니다.

**XAI의 중요성:**

*   **신뢰성 향상**: AI 모델의 결정에 대한 이유를 알 수 있으므로, 사용자는 모델을 더 신뢰할 수 있습니다.
*   **책임감 확보**: AI 모델의 오작동 시 원인을 파악하고 책임을 규명하는 데 도움이 됩니다.
*   **성능 개선**: 모델의 약점을 파악하고 개선하여 전반적인 성능을 향상시킬 수 있습니다.
*   **윤리적 문제 해결**: 편향된 데이터로 인한 불공정한 결정이나 차별적인 결과를 방지할 수 있습니다.

**XAI 주요 기술:**

*   **LIME (Local Interpretable Model-agnostic Explanations)**: 특정 예측에 대한 설명을 제공하는 모델
*   **SHAP (SHapley Additive exPlanations)**: 게임 이론의 Shapley value를 사용하여 각 feature의 중요도를 설명하는 모델
*   **CAM (Class Activation Map)**: CNN 모델에서 특정 클래스를 활성화하는 영역을 시각화하는 기술

**XAI 활용 분야:**

*   **금융**: 신용 평가 모델의 설명 가능성을 높여 공정한 대출 심사 지원
*   **의료**: 진단 모델의 결정 근거를 제시하여 의사의 진단을 보조하고 환자의 신뢰도 향상
*   **법률**: 법률 해석 모델의 판단 과정을 설명하여 법률 전문가의 의사 결정 지원
*   **자율 주행**: 자율 주행 시스템의 판단 근거를 제공하여 안전성 및 신뢰성 확보

XAI는 인공지능 기술의 발전과 함께 더욱 중요해지고 있으며, 다양한 분야에서 AI의 투명성, 공정성, 책임감을 높이는 데 기여할 것으로 기대됩니다.

AutoML(Automated Machine Learning)은 머신러닝 모델 개발의 여러 단계를 자동화하는 기술입니다. 데이터 전처리, 특징 선택, 모델 선택, 하이퍼파라미터 최적화 등을 자동화하여 머신러닝 전문 지식이 없는 사람도 쉽게 모델을 개발하고 배포할 수 있도록 돕습니다.

**AutoML 주요 기능:**

*   **자동 데이터 전처리**: 결측값 처리, 이상치 제거, 데이터 변환 등을 자동으로 수행합니다.
*   **자동 특징 선택**: 모델 성능에 중요한 특징을 자동으로 선택합니다.
*   **자동 모델 선택**: 다양한 알고리즘 중에서 최적의 모델을 자동으로 선택합니다.
*   **자동 하이퍼파라미터 최적화**: 모델 성능을 최적화하는 하이퍼파라미터 값을 자동으로 찾습니다.
*   **모델 평가 및 배포**: 학습된 모델의 성능을 평가하고, 쉽게 배포할 수 있도록 지원합니다.

**AutoML 장점:**

*   **개발 시간 단축**: 모델 개발에 소요되는 시간을 크게 단축할 수 있습니다.
*   **비전문가 활용**: 머신러닝 전문 지식이 없는 사람도 쉽게 모델을 개발할 수 있습니다.
*   **성능 향상**: 사람이 찾기 어려운 최적의 모델과 하이퍼파라미터를 찾을 수 있어 성능 향상에 기여합니다.

**AutoML 활용 분야:**

*   **금융**: 신용 평가, 사기 탐지, 투자 분석 등
*   **마케팅**: 고객 세분화, 예측 분석, 추천 시스템 등
*   **제조**: 품질 관리, 생산 최적화, 설비 고장 예측 등
*   **헬스케어**: 질병 진단, 환자 모니터링, 신약 개발 등

AutoML은 머신러닝 기술의 접근성을 높이고, 다양한 분야에서 AI 활용을 확산시키는 데 기여할 것으로 기대됩니다.

## 기본 개념: 특징 (Feature)과 벡터 (Vector)

머신러닝에서 데이터는 **특징 (Feature)**과 **벡터 (Vector)**로 표현됩니다. 이러한 표현 방식을 통해 데이터를 수치화하고, 모델이 학습할 수 있도록 합니다.

*   **특징 (Feature)**: 데이터의 개별 속성을 나타냅니다. 예를 들어, 사람의 특징으로는 키, 몸무게, 나이 등이 있을 수 있습니다. 각 특징은 데이터의 열 (column)에 해당합니다.
*   **벡터 (Vector)**: 여러 특징들을 모아 놓은 것으로, 데이터 포인트를 나타냅니다. 각 데이터 포인트는 N차원 공간에서의 한 점으로 표현될 수 있으며, 벡터는 이 점의 좌표를 나타냅니다.

## 데이터 기본 용어

*   **행 (Row) → 관측치 (Observation)**: 각 행은 하나의 데이터 샘플 또는 관측치를 나타냅니다. 예를 들어, 한 사람의 키, 몸무게, 나이 등의 정보를 담고 있는 행이 하나의 관측치가 됩니다.
*   **열 (Column) → 특징 (Feature)**: 각 열은 데이터의 속성 또는 특징을 나타냅니다. 예를 들어, 키, 몸무게, 나이는 각각 데이터의 특징이 됩니다.

## 데이터 표현 예시

예를 들어, 3개의 데이터 포인트가 있고, 각 데이터 포인트가 4개의 특징을 가진다고 가정해 봅시다.

*   **특징의 수**: 4개 (예: 키, 몸무게, 나이, 성별)
*   **관측치의 수**: 3개 (3명의 사람)

이 경우, 각 관측치는 4차원 공간에 존재하는 한 점으로 표현됩니다. 각 데이터 포인트는 4개의 요소를 지닌 벡터로 나타낼 수 있습니다. 예를 들어, 첫 번째 사람의 특징이 [180, 75, 25, 1] (키, 몸무게, 나이, 성별)이라면, 이 벡터는 4차원 공간에서 첫 번째 사람의 위치를 나타냅니다.

## 데이터 포인트 간의 거리 측정

데이터 포인트를 벡터로 표현하면, 데이터 간의 거리를 측정할 수 있습니다. 이는 유사한 데이터끼리 그룹화하거나, 서로 구분되는 관측치를 식별하는 데 유용합니다. 거리 측정 방법으로는 유클리드 거리 (Euclidean Distance), 코사인 유사도 (Cosine Similarity) 등이 있습니다.

*   **유클리드 거리**: 두 점 사이의 직선 거리를 측정합니다.
*   **코사인 유사도**: 두 벡터 사이의 각도를 측정하여 유사도를 판단합니다.

이러한 거리 측정 방법을 통해 데이터 분석 및 머신러닝 모델 학습에 활용할 수 있습니다.

## 분포 (Distribution)

분포를 안다는 것은 해당 데이터가 특정 범위 내에 존재할 확률을 계산할 수 있다는 의미입니다. 이를 통해 데이터의 특성을 이해하고 예측 모델을 구축하는 데 활용할 수 있습니다.

### 정규 분포 (Normal Distribution)

정규 분포는 자연 현상에서 가장 흔하게 나타나는 분포 중 하나로, "보통의 분포"라고도 불립니다. 종 모양의 형태를 가지며, 평균을 중심으로 대칭적인 특징을 가집니다.

*   **특징**: 하나의 특징(feature)에 대한 분포를 나타냅니다.
*   **예시**: 키, 몸무게, 시험 점수 등

### 정규 분포의 특징

정규 분포에서 생성된 데이터는 다음과 같은 특징을 가집니다.

*   **1 표준편차 (Standard Deviation) 내**: 전체 데이터의 약 68.2%가 평균으로부터 1 표준편차 거리에 존재합니다.
*   **2 표준편차 내**: 전체 데이터의 약 95.4%가 평균으로부터 2 표준편차 거리에 존재합니다.
*   **3 표준편차 내**: 전체 데이터의 약 99.7%가 평균으로부터 3 표준편차 거리에 존재합니다.

이러한 특징을 통해 데이터의 분포를 파악하고 이상치를 탐지하는 데 활용할 수 있습니다.


이미지 데이터로 차원 연습

일반적으로 이미지 데이터는 다차원 배열로 표현됩니다. 예를 들어, 흑백 이미지는 2차원 배열로, 컬러 이미지는 3차원 배열로 표현됩니다. 이러한 이미지 데이터를 머신러닝 모델에 입력하기 위해서는 데이터를 적절하게 변환해야 합니다.

**예시:**

MNIST 데이터셋은 28x28 픽셀의 흑백 손글씨 이미지로 구성되어 있습니다. 이 데이터셋을 사용하여 차원 변환을 연습해 보겠습니다.

```python
import numpy as np

# MNIST 데이터셋의 이미지 데이터라고 가정
# 실제 데이터는 적절한 방식으로 로드해야 함
독립 = np.random.rand(60000, 28, 28)  # 60000개의 28x28 이미지

print(독립.shape)  # (60000, 28, 28)

# 2차원 배열로 변환 (60000, 784)
독립 = 독립.reshape(60000, 28*28)

print(독립.shape)  # (60000, 784)
```

**설명:**

1.  **`독립 = np.random.rand(60000, 28, 28)`**: 60000개의 28x28 흑백 이미지를 임의로 생성합니다. 실제 MNIST 데이터셋은 `tensorflow`나 `torchvision` 등의 라이브러리를 사용하여 로드해야 합니다.
2.  **`print(독립.shape)`**: `(60000, 28, 28)`이 출력됩니다. 이는 60000개의 이미지가 각각 28x28 픽셀로 구성되어 있음을 의미합니다.
3.  **`독립 = 독립.reshape(60000, 28*28)`**: 3차원 배열을 2차원 배열로 변환합니다. 28x28 픽셀 이미지를 784개의 픽셀 값을 가진 1차원 벡터로 펼치는 것입니다.
4.  **`print(독립.shape)`**: `(60000, 784)`이 출력됩니다. 이는 60000개의 이미지가 각각 784개의 픽셀 값을 가진 벡터로 변환되었음을 의미합니다.

**주의사항:**

*   실제 이미지 데이터를 사용할 때는 데이터의 형태와 범위를 확인하고 적절하게 전처리해야 합니다.
*   컬러 이미지의 경우, 3차원 배열로 표현되며, 각 차원은 높이, 너비, 채널 (RGB)을 나타냅니다.

이러한 차원 변환은 이미지 데이터를 머신러닝 모델에 입력하기 위한 기본적인 전처리 과정입니다.

```python
import numpy as np

# 컬러 이미지 데이터라고 가정
# (이미지 개수, 높이, 너비, 채널)
컬러_이미지 = np.random.rand(1000, 64, 64, 3)  # 1000개의 64x64 컬러 이미지 (RGB)

print(컬러_이미지.shape)  # (1000, 64, 64, 3)

# 4차원 배열을 2차원 배열로 변환 (1000, 64*64*3)
컬러_이미지 = 컬러_이미지.reshape(1000, 64*64*3)

print(컬러_이미지.shape)  # (1000, 12288)
```

**설명:**

1.  **`컬러_이미지 = np.random.rand(1000, 64, 64, 3)`**: 1000개의 64x64 컬러 이미지를 임의로 생성합니다. 각 이미지는 RGB 채널을 가지므로 마지막 차원의 크기는 3입니다.
2.  **`print(컬러_이미지.shape)`**: `(1000, 64, 64, 3)`이 출력됩니다. 이는 1000개의 이미지가 각각 64x64 픽셀로 구성되어 있고, 각 픽셀은 3개의 채널(RGB)을 가진다는 것을 의미합니다.
3.  **`컬러_이미지 = 컬러_이미지.reshape(1000, 64*64*3)`**: 4차원 배열을 2차원 배열로 변환합니다. 각 이미지를 1차원 벡터로 펼치는 것이며, 64\*64\*3 = 12288개의 요소로 구성됩니다.
4.  **`print(컬러_이미지.shape)`**: `(1000, 12288)`이 출력됩니다. 이는 1000개의 이미지가 각각 12288개의 픽셀 값을 가진 벡터로 변환되었음을 의미합니다.

**주의사항:**

*   실제 이미지 데이터를 사용할 때는 데이터의 형태와 범위를 확인하고 적절하게 전처리해야 합니다.
*   이미지 데이터의 차

## 데이터 분석 방법론
데이터 분석 방법론은 어떤 일을 하기 위한 체계적인 절차와 그 처리 방법을 정리한 것입니다. 분석 방법에는 크게 3가지로, 통계적 방법론, 데이터 마이닝 방법론, 빅데이터 방법론 등이 있습니다.
주요 데이터 분석 방법론:
*   KDD (Knowledge Discovery in Databases)
*   CRISP-DM (Cross-Industry Standard Process for Data Mining)
*   SEMMA (Sample, Explore, Modify, Model, Assess)
데이터 분석 방법론은 요리책에 비유할 수 있습니다. 요리책은 특정 요리를 만들기 위한 체계적인 절차와 방법을 제공하며, 데이터 분석 방법론은 데이터 분석 프로젝트를 성공적으로 수행하기 위한 단계를 제시합니다.
(추가적으로) 요리책과 객체지향방법론을 비교하면, 요리책은 정해진 레시피에 따라 요리를 만드는 절차를 설명하는 반면, 객체지향방법론은 재사용 가능한 객체들을 설계하고 조합하여 시스템을 구축하는 방법을 제시합니다. 데이터 분석에서도 유사하게, 정해진 분석 절차를 따르는 방법론과 재사용 가능한 분석 모듈을 활용하는 방법론이 있을 수 있습니다.


KDD 분석 방법론

KDD (Knowledge Discovery in Databases) 분석 방법론은 데이터베이스에서 지식을 발견하는 과정으로, 1996년 Fayyad 등이 제안한 방법론입니다. KDD는 데이터 마이닝을 포함하는 더 넓은 개념으로, 데이터 전처리, 데이터 마이닝, 결과 평가 및 해석 등 전체적인 지식 발견 과정을 다룹니다.

**KDD 주요 단계:**

1.  **이해 (Understanding the Domain):**
    *   분석 목표 정의: 어떤 문제를 해결하고 싶은지, 어떤 지식을 발견하고 싶은지 명확하게 정의합니다.
    *   배경 지식 습득: 해당 분야에 대한 기본적인 이해를 쌓고, 관련 정보를 수집합니다.
2.  **데이터 선택 (Data Selection):**
    *   데이터 소스 결정: 분석에 필요한 데이터를 어디에서 얻을 수 있는지 결정합니다.
    *   데이터 선택: 분석 목표에 맞는 데이터를 선택합니다.
3.  **데이터 전처리 (Data Preprocessing):**
    *   데이터 정제 (Cleaning): 결측값 처리, 이상치 제거, 중복 데이터 제거 등 데이터를 정리합니다.
    *   데이터 변환 (Transformation): 데이터를 분석에 적합한 형태로 변환합니다 (예: 정규화, 표준화).
4.  **데이터 마이닝 (Data Mining):**
    *   모델 선택: 분석 목표에 맞는 데이터 마이닝 기법 (예: 분류, 회귀, 군집화)을 선택합니다.
    *   알고리즘 선택: 선택한 데이터 마이닝 기법에 맞는 알고리즘을 선택합니다 (예: 의사결정 트리, 신경망, K-평균).
    *   모델 학습: 데이터를 사용하여 모델을 학습시킵니다.
5.  **평가 (Evaluation):**
    *   모델 평가: 학습된 모델의 성능을 평가합니다.
    *   결과 해석: 모델의 결과를 해석하고, 의미 있는 지식을 발견합니다.
6.  **지식 표현 (Knowledge Representation):**
    *   발견된 지식을 시각화하거나, 보고서 형태로 정리하여 표현합니다.
    *   표현된 지식을 의사 결정에 활용합니다.

KDD 분석 방법론은 데이터 분석 프로젝트를 체계적으로 수행하기 위한 가이드라인을 제공하며, 데이터로부터 의미

#CRISP-DM 분석 방법론

CRISP-DM (Cross-Industry Standard Process for Data Mining)은 데이터 마이닝 프로젝트를 위한 표준 방법론입니다. 6개의 주요 단계를 포함하며, 각 단계는 순차적일 수도, 반복적일 수도 있습니다.

1.  **업무 이해 (Business Understanding):**
    *   프로젝트 목표 정의 및 성공 기준 설정
2.  **데이터 이해 (Data Understanding):**
    *   데이터 수집, 특성/품질/분포 분석
3.  **데이터 준비 (Data Preparation):**
    *   데이터 정제, 변환, 통합
4.  **모델링 (Modeling):**
    *   모델 선택, 구축, 튜닝
5.  **평가 (Evaluation):**
    *   모델 성능 평가, 결과 검증 및 개선
6.  **배포 (Deployment):**
    *   모델 운영 환경 배포, 모니터링 및 유지 보수

SEMMA (Sample, Explore, Modify, Model, Assess) 방법론에 대한 설명을 제공해 드리겠습니다.

SEMMA는 SAS Institute에서 개발한 데이터 마이닝 방법론으로, 데이터 분석 과정을 5단계로 단순화한 것이 특징입니다. 이 방법론은 데이터 과학자가 데이터에서 가치 있는 통찰력을 추출하는 데 도움이 되는 체계적인 접근 방식을 제공합니다.

1.  **Sample (샘플):**
    *   대규모 데이터 세트에서 분석에 사용할 데이터 샘플을 선택합니다.
    *   데이터의 대표성을 확보하기 위해 다양한 샘플링 기법을 활용할 수 있습니다.
2.  **Explore (탐색):**
    *   데이터의 기본적인 통계적 속성을 파악하고, 시각화를 통해 데이터의 패턴과 이상치를 탐색합니다.
    *   데이터의 분포, 상관 관계 등을 분석하여 데이터에 대한 이해도를 높입니다.
3.  **Modify (수정):**
    *   결측값 처리, 이상치 제거, 변수 변환 등 데이터 품질을 개선하기 위한 전처리 작업을 수행합니다.
    *   모델링에 적합한 형태로 데이터를 변환합니다.
4.  **Model (모델):**
    *   데이터 마이닝 기법 (분류, 회귀, 군집화 등)을 선택하고, 적절한 알고리즘을 적용하여 모델을 구축합니다.
    *   다양한 모델을 시도하고, 성능을 비교하여 최적의 모델을 선택합니다.
5.  **Assess (평가):**
    *   구축된 모델의 성능을 평가하고, 결과를 해석합니다.
    *   모델의 예측 정확도, 일반화 성능 등을 평가하고, 필요에 따라 모델을 개선합니다.
    *   평가 결과를 바탕으로 모델의 실질적인 가치를 판단합니다.

SEMMA 방법론은 비교적 단순하고 직관적인 단계로 구성되어 있어 데이터 마이닝 초보자도 쉽게 이해하고 적용할 수 있습니다. 또한, 각 단계를 반복적으로 수행하면서 모델의 성능을 지속적으로 개선할 수 있습니다.
#CRISP-DM
## 대립가설과 귀무가설

**귀무 가설 (Null Hypothesis, H0):**

- 기존에 받아들여지던 사실 또는 효과가 "없다"라는 가설
- 통계적 검

[실행 관련 단축키]

1. Ctrl + Enter = 해당 셀을 실행하고 커서를 해당 셀에 두는 경우 (결과 값만 보고자 할 때)
2. Shift + Enter = 해당 셀을 실행하고 커서를 다음 셀로 넘기는 경우 (여러가지 값을 빠르게 출력할 때)
3. Alt + Enter = 해당 셀을 실행하고 셀을 삽입한 후 커서를 삽입한 셀로 넘기는 경우 (다음 작업 공간이 없을 때)

  

[셀 삽입/삭제 관련 단축키]
Ctrl + M A = 코드 셀 위에 삽입
Ctrl + M B = 코드 셀 아래 삽입
Ctrl + M D = 셀 지우기
Ctrl + M Y = 코드 셀로 변경
Ctrl + M M = 마크다운 셀로 변경
Ctrl + M Z = 실행 취소


Agile의 등장 배경

| 원인            | 설명                                                  |
| ------------- | --------------------------------------------------- |
| S/W개발 환경의 변화  | 정보시스템에 대한 사용자 요구 다양                                 |
| 기존 개발 방법론의 한계 | 문서 위주, 절차중심의 기계적인 방법론으로는 빠른 변화 대을과 효율적인 시스템 개발이 어려움 |
|               |                                                     |

SDLC

소프트웨어 개발 타당성 검토로부터 개발, 유지보수, 폐기까지의 전 과정을 하나의 생명주기로 정의하고 단계별 공정을 체계화한 모델

S/W Engineering 원리를 SW 생명주기에 적용하여 작업절차, 방법

